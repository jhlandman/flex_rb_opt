% ----------------------------------------------------------------
% AMS-LaTeX definitions for a Dissertation/Thesis ****************
% ----------------------------------------------------------------
\documentclass[a4paper,11pt,nocenter,bold,noupper,headcount]{mythesis}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dashrule}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{color}
\usepackage{epsfig}
\usepackage{natbib}
\usepackage{appendix}
\usepackage{caption}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{bbm}
\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{palatino}
\usepackage{hyperref}
\usepackage{tabu}
\usepackage{float}
\hypersetup{%
  colorlinks=true,% hyperlinks will be coloured
  linkcolor=blue,% hyperlink text will be green
  citecolor=blue
}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


% THEOREMS -------------------------------------------------------

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ass}[thm]{Assumption}
\newtheorem{exam}[thm]{Example}

% OTHER ----------------------------------------------------------

\newcommand{\setlinespacing}[1]
           {\renewcommand{\baselinestretch}{#1}\small\normalsize}

% PREAMBLE -------------------------------------------------------

\title{Flexible risk-based portfolio optimisation}

\author{Jayson Landman \\ \small{Supervisors: Obeid Mahomed \& Emlyn Flint} }

\statement{A dissertation submitted to the Faculty of Commerce, University of Cape Town, in partial fulfilment of the requirements
for the degree of Master of Philosophy.}

\institution{MPhil in Mathematical Finance,\\
University of Cape Town.\\
$ $\\
$ $\\
\begin{figure}[h!]
\begin{center}
\includegraphics[width=4cm]{UCTLogo.jpg}
\end{center}
\end{figure}
}

\chapapp{\chaptername}
\chapterfont{\Huge\bfseries}
\setlength{\topmargin}{0.5cm} \setlength{\headsep}{1cm}
\setlength{\textwidth}{14.0cm} \setlength{\headwidth}{14.0cm}
\setlength{\textheight}{22cm} %\setlength{\leftmargin}{0cm}
\setlength{\oddsidemargin}{1cm} \setlength{\evensidemargin}{0cm}

%%% --------------------------------------------------------------

\begin{document}

\maketitle \setlinespacing{1.20} \frontmatter

\include{FrontMatter}

\setlinespacing{1} \thispagestyle{plain}
\tableofcontents \thispagestyle{plain}
\listoffigures \thispagestyle{plain}
\listoftables \thispagestyle{plain}
\setlinespacing{1.20}

% ----------------------------------------------------------------

\mainmatter

% The body of the dissertation/thesis should be added here! Note
% that it is probably better to include the chapters as files using
% \include{Chapter1} to include a file called Chapter1.tex. Each
% chapter should start with \chapter{Chapter Name}, with sections
% being added by \section{Section Name}.

%\include{Chapter1}


% ------------------
\chapter{Introduction}


If a risk-averse investor wants to construct portfolios with desirable properties, they would ideally want to find allocations that offer an attractive risk-reward trade-off. \cite{M52} developed modern portfolio theory and introduced the mean-variance optimal portfolio as a quantitative solution to this asset allocation problem. However, the reward derived from this portfolio has to be estimated from sample data and is often difficult to accurately predict - which, in turn, leads to Markowitz's mean-variance portfolio being highly sensitive to the estimated portfolio inputs.

As an alternative, risk-based investing provides an avenue for finding portfolios for which expected returns do not need to be estimated, and therefore resolves the portfolio expected return sensitivity problem. Examples of risk-based portfolios commonly seen in practice are the global minimum variance portfolio, the equal risk contribution portfolio, and the equal weight portfolio. These three portfolios are optimal for investors that prioritise weight diversification, risk diversification, or a specific combination of both. 

Nevertheless, risk-based portfolios remain sensitive to covariance matrix estimation and hence estimation risk. Improving risk-based estimation is done in three ways in this research. The first improvement alters the covariance estimation procedure by accounting for differences in the sample data. These changes include grouping to account for both non-normality and state-based inhomogeneity. The second involves penalising the optimisation to limit the range of admissable portfolios, which increases the investor's odds of choosing a well-estimated portfolio. The final enhancement changes the implementation methodology entirely by performing the portfolio optimisation on subsets of assets and then resampling to find an aggregate portfolio.

This research aims to bring together useful elements of risk-based portfolio estimation and construction methodology into a single flexible framework. The general structure allows a choice of risk-based portfolio as well as estimation risk reduction technique to improve the out-of-sample portfolio performance. Once we have established a framework, the specific portfolio and estimation technique, examples are developed theoretically. All of these reforms will be hollow without being applied to actual financial data. Therefore, the various estimation techniques and risk-based portfolio pairs are back-tested using South African equity data in an experiment, with the results being measured by standard performance methodologies.


This research is built on the work of several different authors. Firstly - as with nearly all portfolio construction research - this dissertation hinges on the modern portfolio theory of \cite{M52}. It then considers the particular case of risk-based investing and makes use of the generalised frameworks introduced by \cite{J13} and \cite{RR15}. Finally, in terms of improving the estimation and optimisation processes, we make use of the ideas investigated by \cite{FD18}, \cite{K18}, and \cite{SW17}.

The rest of this dissertation is set out as follows. Chapter 2 outlines a general framework for constructing risk-based portfolios and estimating them in a robust manner. Chapter 3 gives an overview of the specific risk-based portfolios considered in this work. Chapter 4 presents several techniques for reducing estimation risk, exploring their theoretical underpinnings and providing general intuition. Chapter 5 then considers the empirical application of these techniques, highlighting several technicalities. Chapter 6 then applies the flexible risk-based framework to SA equity data, providing an empirical comparison of different implementations. Chapter 7 concludes the research and provides avenues for further research.


\chapter{A framework for constructing risk-based portfolios} \label{gen_frame_ch}

This chapter introduces risk-based investing against the backdrop of traditional mean-variance optimal portfolio construction techniques. Once its place in the literature is evident, a general framework for risk-based investors is derived. This general structure allows the user to determine which risk-based portfolio they would like to work with and also provides flexibility in terms of how they would like to estimate it.

\section{An overview of modern portfolio theory} \label{MPT}
Every investor has a universe of $N$ assets to which they can allocate capital. The proportion of their allocation to the \textit{i}$^{\text{th}}$ asset, $w_i$, depends on the investor's risk and return preferences. They could prefer riskier asset combinations because they require high capital growth, or they could prefer more stable asset combinations that prioritise capital preservation. The column vector $w = [w_1,w_2,â€¦,w_N]^\intercal$ is an $N \times 1$ vector of allocations that define an investor's portfolio. This portfolio is constrained by the investor's capital budget, which may be articulated through the notion of weights. Therefore, all considered portfolios should adhere to the budget constraint: $\sum_i w_i =  1$.

In his seminal paper, \cite{M52} attempts to quantify the asset allocation process. Herein he posits that the investor has to make a risk-reward trade-off when considering their portfolio returns, $R_\mathrm{p}$, over a pre-determined time horizon $[0, T]$. In his framework, he measures risk with the variance of portfolio returns $\mathbb{V}\mathrm{ar}[R_\mathrm{p}] = w^\intercal \Sigma w$, where $\Sigma$ is the $N \times N$ asset return covariance matrix. Markowitz measures reward by the expectation of portfolio returns $ \mathbb{E}[R_\mathrm{p}] = w^\intercal \mu$, where $\mu$ is the $N \times 1$ vector of expected asset returns. If the investor fixes expected returns to a constant, $\mathbb{E}[R_\mathrm{p}] = c$, they encounter the problem of minimising portfolio risk, $\mathbb{V}\mathrm{ar}[R_\mathrm{p}]$. The mean-variance optimal (MVO) portfolio $w_\mathrm{mvo}$ achieves this goal while adhering to the budget constraint and the fixed expected return constraint. Written mathematically, $w_\mathrm{mvo}$ is the solution to the following Markowitz optimisation:
\begin{align} \label{eqn_mvo}
w_\mathrm{mvo} &= \underset{w}{\text{argmin}} \Big \{ w^\intercal \Sigma w \Big \},
\end{align}
subject to the constraints:
\begin{align*}
\mathcal{C}(w) &= 
\begin{cases}
 w^\intercal \mu &= c \;\;\;  \\
w^\intercal \underline{1} &= 1 \;\; .
\end{cases}
\end{align*}

There is a complication when using this framework in practice because $\Sigma$ and $\mu$ are unknown. The investor can only observe a small set of sample returns from the underlying stock processes that use these quantities as inputs. The sample returns, represented by a $N \times T$ matrix $\textbf{X}$, can be used to estimate $\Sigma$ and $\mu$. The sample estimates are:
\begin{align}
\textbf{S}&=  \frac{1}{T - 1}(\textbf{X} - \mathbf{\bar{x}} \underline{1}^\intercal_T)(\textbf{X} - \mathbf{\bar{x}} \underline{1}^\intercal_T)^\intercal, \\
\mathbf{\hat{\mu}} &= \mathbf{\bar{x}} ,
\end{align}
where $\mathbf{\bar{x}}$ is a $N \times 1$ vector of mean returns. A total of $\frac{N(N+1)}{2}$ distinct parameters are being estimated for the sample covariance matrix (SCM) while $N$ distinct sample expected returns are estimated. $\textbf{S}$ and $\hat{\mu}$ can be substituted into equation (\ref{eqn_mvo}) to infer an MVO portfolio, $\hat{w}_\mathrm{mvo}$. For each level of portfolio return $c = \hat{w}^\intercal \hat{\mu} $, an estimated portfolio volatility level $\mathbb{SD}[\hat{R}_\mathrm{p}] = \sqrt{\hat{w}^\intercal\textbf{S}\hat{w}}$ is realised. These estimated expected return-volatility pairs induce a frontier in the expected return-volatility plane, which may or may not be close to the frontier implied by the actual inputs $\Sigma$ and $\mu$. The actual frontier is optimal for the Markowitz framework, and he terms it the `efficient frontier'.

The above framework and estimation procedure yields a solution to the asset allocation dilemma, but there is still a sensitivity predicament. \cite{M89} shows that the MVO procedure, as described above, overweights assets with substantial estimated returns $\mathbf{\bar{x}}$. However, these are the same assets that are likely to have been misestimated. Thus, any potential estimation errors are `maximised' - an undesirable property that makes $\hat{w}_\mathrm{mvo}$ a potential liability for the investor to hold.

The MVO procedure is commonly adjusted to reduce sensitivity to the input $\mathbf{\bar{x}}$ in one of two ways. The first is to estimate expected returns in a manner that targets return drivers or factors, which has motivated the rise of factor-based investing \citep{A14}. The second adjustment is to remove the dependency on expected return estimates altogether and only construct portfolios based on their risk properties. The latter has inspired the field of risk-based investing and is the focus of this dissertation. 

\section{Introducing risk-based investing} 

A general risk-based portfolio optimisation programme is given below in equation (\ref{eqn_rb}). Removing the expected return constraint and altering the previous MVO optimisation problem for the consideration of a generalised objective risk function $f(\cdot| \textbf{X})$ yields:
\begin{align} \label{eqn_rb}
w^* = \underset{w}{\text{argmin}} \Big \{ f(w, \Sigma| \textbf{X}) \Big \},
\end{align}
subject to the constraint:
\begin{align*}
w^\intercal \underline{1} &= 1 \;\; ,
\end{align*}

where $f(\cdot | \textbf{X})$ is a risk metric to be minimised. The choice of $f(\cdot | \textbf{X})$ determines which risk-based portfolio is the optimal solution. Chapter \ref{rb_ports} expounds on the risk-based portfolio types relevant to this research.

The range of feasible portfolios given by equation (\ref{eqn_rb}) is practically too general because unlikely single asset weights are still possible. To this end, an investor should apply a weight constraint to limit feasible allocations, ensuring comparability with practical investing. \cite{JM03} show that risk-based long-short portfolios can have extreme weights in practice, which are unlikely to be accepted by an investor. 

Additionally, \cite{JM03} also conclude that imposing the long-only investment constraint on US equities leads to improved efficiency for optimal portfolios constructed with the first two sample moments $\hat{\mu}$ and $\textbf{S}$. Hence, applying the long-only constraint is both statistically appropriate and practically relevant for most investors.

Equally important is that an investor will be reluctant to concentrate their portfolio in a small number of assets. Limiting the maximum single asset allocation to a selected weight $\alpha$ avoids such concentration. These constraints are concurrently expressed as $\{w :0 \leq w_i \leq \alpha, \; \forall i \}$ and can be added to optimisation (\ref{eqn_rb}). In its current form, the developed framework is still somewhat abstract, so it is not obvious how to improve it. Even so, one can always define specific properties that the framework ought to have for there to be a good chance of it operating as intended.

\section{Improving risk-based portfolios} \label{imp_rb}

\cite{H23} defines a mathematical problem as `well-posed' if:
\begin{enumerate} [label=(\roman*)]
\itemsep0em 
	\item the solution exists;
	\item the solution is unique;
	\item the solution is not overly sensitive to small perturbations in inputs.
\end{enumerate}
Well-posed problems are easier to work with and are more stable than ill-posed problems - ones that fall short of the definition. The expected return constraint was previously disregarded for MVO portfolios because the MVO framework often does not meet requirement (iii) when the sample mean returns estimate $\mu$. Risk-based portfolios are therefore more `well-posed' than MVO portfolios.

However, there are two common ways in which risk-based portfolios are also ill-posed. The first is if there are very few sample observations; namely, if $T < N$. In this scenario, the covariance matrix is not of full rank and is therefore not invertible, causing non-unique solutions to $w^*$. The framework is then ill-posed by (ii). The second is if small changes in $\textbf{X}$ result in large deviations of $w^*$. The framework is then ill-posed by (iii). Many researchers such as \cite{JK81} and \cite{B91} have shown that the latter phenomenon is observed in practice, and often persists even if $T$ is much larger than $N$.  To address point (iii) and make the problem well-posed, one needs a measure of sensitivity. We outline a pseudo-derivation of a sensitivity measure below.

\cite{K18} views the portfolio optimisation problem from a modelling perspective. The returns on the portfolio are modelled directly by an unknown function $g(\cdot)$:
\begin{align} \label{ret_mod}
R_p & = g(\textbf{X}) + \epsilon,
\end{align}
where $\epsilon$ has the normal distribution with zero mean and variance $\phi^2$. Estimating $g(\cdot)$ is the aim of using the framework, and it is a real-valued function that is not necessarily differentiable or continuous. Each algorithm $q$ refers to a combination of estimation procedure for the inputs, and a computation of a risk-based portfolio using equation (\ref{eqn_rb}). Each $q$ specifies an estimate of the function $g$, denoted $\hat{g}_q$. In the same way that $\hat{w}^*$ ($w^*$ calculated with sample inputs $\textbf{S}$ and $\hat{\mu}$) can be used to estimate the out-of-sample risk-based portfolio, $\hat{g}_q$ can predict out-of-sample portfolio returns, which are forecasted most accurately by the unobservable function $g$.

It is necessary to distinguish between the two types of data that are available. The first is historical data comprising of the matrix $\textbf{X}_0$ and in-sample returns $R_{\mathrm{p}, 0}$, which combine to form the set $\mathcal{H}_0$. The second is out-of-sample data $\textbf{X}_1$ and $R_\mathrm{p, 1}$, which combine to form the set $\mathcal{H}_1$. Algorithm $q$ does not utilise the data contained in $\mathcal{H}_1$, which are chronologically realised after the most recent points in $\mathcal{H}_0$. A mean squared error penalty is appropriate to measure the accuracy with which $\hat{g}_q (\textbf{X}_1)$ (estimated using $\mathcal{H}_0$) predicts the out of sample returns $R_\mathrm{p, 1}$. \cite{K18} terms the expectation of the out-of-sample mean squared error ``generalisation error" (GE), shown mathematically as:
\begin{align}
GE(\hat{g}_q) & = \mathbb{E}[(R_\mathrm{p, 1} - \hat{g}_q (\textbf{X}_1))^2|{\mathcal{H}_1}],
\end{align}
where GE is specific to a sample. The actual quantity of interest is the expected performance of $q$ for many potential sample sets, as we are evaluating $q$'s efficacy holistically. This quantity is called the expected generalisation error across all samples, denoted $G_q$. Furthermore, $G_q$ may be decomposed\footnote{The decomposition is shown by \cite{FHT01}.} to reflect a common modelling trade-off between bias and variance:
\begin{align}
G_q &:= \mathbb{E} \Big [ \mathbb{E}[(R_\mathrm{p, 1} - \hat{g}_q (\textbf{X}_1))^2|{\mathcal{H}_1}] \Big | \mathcal{H}_0\Big ] , \\
& = \underbrace{ \Big (g(\textbf{X}_1) - \mathbb{E}[\hat{g}_q (\textbf{X}_1)|\mathcal{H}_0] \Big )^2}_{\text{squared bias}} + \underbrace{\mathbb{V}\mathrm{ar} [\hat{g}_q(\textbf{X}_1)|\mathcal{H}_0]}_{\text{variance}} + \underbrace{\phi^2}_{\text{irreducible error}}.\label{eqn_b_v_decomp}
\end{align}
The squared bias is the extent to which the expectation of predicted returns differs from the best possible predictor of returns, the correct function $g(\cdot)$. The variance measures the magnitude by which the predicted returns will vary under repeated sampling. By setting $\hat{g}_q(\cdot) = g(\cdot)$ in equation (\ref{eqn_b_v_decomp}) only statistical noise remains; hence, the noise is irreducible. The risk of misestimating $g(\cdot)$ should not include the risk that is retained by even the best estimator. Therefore, estimation risk is considered as the sum of the first two terms only, the squared bias and the variance. An over-fitted algorithm will have high variation for repeated samples. An under-fitted algorithm will have high bias and be consistently poor for repeated samples. The over- and under-fitting trade-off is an example of how the bias-variance trade-off works in practice.

Until now, we have assumed that the estimated portfolio $\hat{w}^*$ from equation (\ref{eqn_rb}) is an unbiased estimate of the actual risk metric-minimising portfolio $w^*$ because the choice of $f(\cdot | \textbf{X})$ determines precisely the type of risk-based portfolio. However, $f(\cdot|\textbf{X})$ does not precisely determine the estimation risk. Employing a penalty on the objective function introduces bias to reduce estimation risk, \textit{i.e.} hopefully, the squared bias increase does not outweigh the variance decrease. The introduction of the penalty yields an estimated portfolio $\hat{w}^*$ that is consistently closer to $w^*$ than an unbiased portfolio would be. The penalty constraint can be stated as $P(w) \leq s$ and reduces the set of all possible portfolios. If done correctly allocations that are misestimated by the heftiest margins are excluded by this constraint, and allocations that are consistently closer to the actual portfolio remain. The general risk-based framework can now be restated as below to accommodate the penalty using a Lagrangian multiplier approach:
\begin{align} \label{eqn_general}
w^* = \underset{w}{\text{argmin}} \Big \{ f(w, \Sigma| \textbf{X}) + \lambda P(w) \Big \},
\end{align}
subject to the constraints:
\begin{align*}
\mathcal{C}(w) &= 
\begin{cases}
w^\intercal \underline{1} = 1 \;\; \\
0 \leq w_i \leq \alpha, \; \forall i \;\;\; ,
\end{cases}
\end{align*}

where $P(\cdot)$ is the penalty function, and $\lambda$ is the Lagrangian multiplier. Kinn proposes estimating $\lambda$ in a way that is consistent with the rest of the optimisation. Therefore, we apply the $\lambda$ estimation that minimises the portfolio specific risk metric using in-sample data. Additionally, by setting $\lambda =  0$, the unpenalised portfolio can still be recovered. Optimisation (\ref{eqn_general}) will be the general risk-based portfolio optimisation going forward. 

Improving on the vanilla risk-based optimisation is done in three ways in this research, and these improvements are also common in the literature. Approach one deals with the assumption in equation (\ref{ret_mod}) that $\epsilon$ has a normal distribution, and that the errors through time are independent and identically distributed. If $\hat{g}_q$ fails to approximate $g$ correctly, then the assumption is violated. There could be natural heterogeneity in the data accounted for by $g$. \cite{FD18} attempt to deal with heterogeneity by changing the estimation procedure of ${g}$ so that it includes and accounts for potential differences in each observation of sample data. They do this through the application of regimes and quantiles, grouping the input data to increase the accuracy of the estimated model. The second approach adapted from \cite{K18} has already been shown above and involves penalising the optimisation and setting $\lambda > 0$ in equation (\ref{eqn_general}). The third and final approach requires resampling from the observations in $\textbf{X}$ for different subsets of assets and hinges on implementation adjustments. \cite{SW17} suggest finding $w^*$ multiple times for different resampled subsets and blending the result afterwards to find an aggregate weight, that hopefully reduces estimation risk. Chapter \ref{ge_reduce} contains a more detailed exploration of these techniques. Before that, further investigation of desirable risk properties and the portfolios that have them is required to specify $f(\cdot|\textbf{X})$.

\chapter{Overview of risk-based portfolios}  \label{rb_ports}

There are many types of risk-based portfolios, for a broader analysis refer to \cite{DV17}. In this chapter, we review three risk-based portfolios: the global minimum variance, equal weight and equal risk contribution portfolios. The risk properties that these portfolios have, the strategies that bear them out and the conditions under which they perform optimally are all covered.

\section{Risk-based portfolio types}
Each risk-based portfolio is optimal in some sense because they all minimise a specific objective risk function. The nature of the objective function depends on the investor's risk preferences. In line with \cite{M52}, a genetic risk metric to minimise is the portfolio variance, as an investor may not be willing to tolerate large swings in capital value. Setting $f(\cdot|\textbf{X}) = w^\intercal \Sigma w$ in portfolio optimisation (\ref{eqn_general}) yields the global minimum variance (GMV) portfolio, denoted $w_\textrm{gmv}$. The objective function is the same as for the MVO portfolio for an imputed value of $c$, meaning that the GMV portfolio sits on the efficient frontier. The GMV portfolio is also the only risk-based portfolio that is always on the efficient frontier.

\cite{MRT10} utilise the concept of an asset's marginal risk contribution (MRC) in a portfolio to perform risk-based portfolio calculations. It is the sensitivity of the portfolio volatility to the weight of an asset in a portfolio. Alternative representations of the MRC and an outline of why the weighted sum of the MRC's is minimised for the GMV portfolio are shown in appendix \ref{MRC_app}. Below is the mathematical definition of the MRC for the $i^{\mathrm{th}}$ asset:
\begin{align}
\frac{\partial}{\partial w_i} \Big [ \sigma(w) \Big ]= \frac{(\Sigma w)_i}{\sqrt{w^\intercal \Sigma w}}.
\end{align}

When taking the realised portfolio returns as the single risk factor, there is no idiosyncratic risk present for assets in the portfolio against this factor. Accordingly, the GMV portfolio is the lowest possible beta portfolio. The GMV portfolio is, therefore, optimal for the investor that always takes on the least risk at the margin. However, risk has to be estimated, so the investor only knows what the least risky options available to them are in a historical sense. The downside is that always choosing the lowest marginal risk contributing asset is implicitly displaying a high level of confidence in estimations of risk.

Contrastingly, the investor may be at a loss when estimating risk. The only measure of diversification for such an investor is weight diversification. To minimise their risk taken at the margin, this investor would hold the smallest possible weight in each of the assets while still satisfying the constraints. This strategy ensures that the investor avoids the maximum marginal risk contributing asset to the greatest extent possible given that they are unsure which of the $N$ assets it is. The least weight concentrated portfolio that this strategy alludes to is the equal weight (EW) portfolio:
\begin{align}
w_{ew} & = \Big [\frac{1}{N} \; \cdot \cdot \cdot \frac{1}{N} \Big ]^\intercal \; ,
\end{align}

where the weight diversification measure that this portfolio maximises is the inverse Herfindahl index (IHI), calculated as $H^{-1} (w) = (\sum_{i = 1}^N w_i^2)^{-1}$. The most weight concentrated portfolio is the single asset portfolio - where a single non-zero asset weight is $1$. The IHI of this portfolio is $1$, which is as low as it can be. On the other hand, the EW portfolio has an IHI value of $N$. All portfolios, therefore, have an IHI on the interval $[1, N]$. In the presence of a maximum weight constraint, the lower bound of the interval changes. The new lower bound is derived in appendix \ref{IHI_max}. To find the EW portfolio using framework (\ref{eqn_general}), set the objective function to the Herfindahl index, $f(\cdot|\textbf{X}) = \sum_{i = 1}^N w_i^2$. No parameters in the objective function require estimation, so this is a sample return independent optimisation, which reflects the investor's lack of confidence in historical data.

The GMV and EW portfolios represent two extremes of investors, specifically those that value volatility reduction only and those that value weight diversification only. The inequality $\sigma(w_{gmv}) \leq \sigma(w^\diamond) \leq \sigma(w_{ew})$ verifies this intuition. The portfolio $w^\diamond$ has intermediate weight concentration. Proof of the inequality is given in appendix \ref{vol_ineq}. It is unlikely that a capital allocator will completely disregard one risk-based approach for another. One method to construct a sound intermediate portfolio that incorporates the MRC philosophy from the GMV portfolio construction and the high risk-asset avoidance philosophy from the EW portfolio construction is to equalise the total risk contribution (TRC) from each asset. An asset's TRC is the product of its MRC and its weight in the portfolio:
\begin{align*}
\text{TRC}_i &= w_i \cdot \frac{\partial}{\partial w_i} \Big [ \sigma(w)\Big ], \\
& = \frac{w_i \cdot (\Sigma w)_i}{\sqrt{w^\intercal \Sigma w}},
\end{align*}
where TRC's sum to the portfolio volatility. An equal risk contribution (ERC) portfolio equalises all of the TRC's so that no single asset is a comparatively significant contributor to risk. The choice of $f(\cdot|\textbf{X})$ that minimises the squared distances between the TRC's to the greatest extent possible is given below:
\begin{align}
f(w, \Sigma|\textbf{X}) &= \sum_{i = 1}^N  \sum_{j \geq i}^N(w_i(\Sigma w)_i - w_j(\Sigma w)_j)^2 \;\;  .
\end{align}

\cite{MRT10} show that a log-constraint on the weights in GMV optimisation could equivalently express this choice of $f(\cdot| \textbf{X})$ - an idea explored further in appendix \ref{vol_ineq}. Therein it is shown that the ERC portfolio is an intermediate portfolio $w^\diamond$. While there are several other risk-based portfolios that have been suggested in the literature, we will focus our attention only on these three portfolios, which are arguably some of the most common risk-based portfolios seen in practice \citep{J13}. 

\section{Risk-based portfolio properties}

Section \ref{MPT} introduces the Markowitz efficient frontier. \cite{S64} extends this work to deduce that there is an optimal portfolio called the tangency portfolio. He does make certain assumptions about investors' preferences and the presence of a risk-free asset. The market-weighted (MW) portfolio is the portfolio for which the efficient frontier is tangential to the line bisecting the y-axis at the risk-free rate ($r_\mathrm{f}$) in the expected return-volatility plane. The MW portfolio is the portfolio held by all investors in the market on average and is relevant because it offers the investor diversification with negligible transaction costs \citep{P07}. The MW portfolio is not risk-based in the traditional sense, but it does not require an estimate of expected returns to calculate; hence, the MW portfolio offers a cheap benchmark against which to compare risk-based portfolio performance. However, the holder of the MW does implicitly adopt all investors' weighted expectations of expected returns \citep{H91}. The tangency portfolio is optimal for the  Sharpe ratio (SR) measure under Sharpe's assumptions. The measure is defined as:
\begin{align}
\text{SR}_\mathrm{p} = \frac{\mathbb{E}[R_\mathrm{p}] - r_\mathrm{f}}{\mathbb{SD}[R_\mathrm{p}]} \;.
\end{align}

Within the MVO construction, the MW portfolio has the maximum Sharpe ratio (MSR) and is, therefore, the MSR portfolio. \cite{S07} shows that the MSR portfolio, $w_\mathrm{msr}$, can alternatively be expressed as the portfolio for which marginal excess returns and the MRC's are equal for all portfolio constituents. \cite{J13} use this fact to find MSR optimality conditions for each of the risk-based portfolios, some examples of which are shown in appendix \ref{msr_rb}.

Table \ref{tab_rb_port} summarises the salient risk properties of the EW, ERC, and GMV portfolios. Included is the strategy to find the portfolio, the requirements for when the portfolio coincides with the MSR portfolio, and their empirical risk characteristics. The risk characteristics entail whether the risk is inherent to the investment, the construction of the portfolio, or liquidity restrictions when creating the portfolio.


\begin{table} [h] 
\caption{Risk-based investing portfolio properties \citep{J13}.}
\label{tab_rb_port}
\centering
 \begin{tabular}{p{1.5cm} p{1.9cm} p{3.9cm}  p{4.4cm}  } 
\hline
\textbf{Portfolio} &  \textbf{Strategy} & \textbf{MSR conditions} & \textbf{Risk characteristics}\\ 
 \hline
EW & Equalise $w_i$ & Identical \newline excess returns. \newline Identical volatilities.  \newline Identical correlations. & Medium to high risk. \newline Insensitive to $\Sigma$. \newline Low turnover. \\
&&&\\
ERC & Equalise $\text{TRC}_i$ & Identical \newline Sharpe ratios. \newline Identical correlations. & Medium risk. \newline Moderately sensitive to $\Sigma$. \newline Medium  turnover. \\
&&&\\
GMV & Equalise $\text{MRC}_i$ & Identical \newline excess returns. & Lowest risk. \newline Highly sensitive to $\Sigma$. \newline High turnover.  \\
 \hline
\end{tabular}
\end{table}

While the MSR conditions are theoretically compelling, out-of-sample optimality is harder to determine in practice. \cite{H91} show that portfolios that are superior to the MW portfolio exist when: short-selling is restricted, investments are taxed, and foreign investors are active market participants. These portfolios should have the same expected return as the MW portfolio with lower volatility. Their statement is true even in an `efficient market'. Studies of the historical performance show that some portfolios outperform others. In these studies, the authors restrict the asset universe to US equities; hence, their results will not necessarily translate to South Africa. The hope of introducing risk-based portfolios is to find Haugen and Baker's superior portfolios. Evidence supporting this ambition exists. \cite{DGL07} demonstrate the robust out-of-sample performance of EW portfolios when compared to MVO and MW portfolios for a broad range of asset universes. \cite{C06} also demonstrate that GMV portfolios show outperformance against the MW and MVO benchmarks. They initially attribute this to the diachronic persistence of covariances when compared to expected returns. In a later paper, \cite{C11} suggest that the outperformance is due to a bias inherent in the portfolio construction towards stocks that do not move with the rest of the market, but that still have comparatively high expected returns.

Within risk-based portfolios, \cite{K10} have shown that GMV portfolios outperform EW portfolios when the implementer uses a long enough estimation window. Therefore, they establish a defence for using optimisation on a sample covariance matrix. This research remains consistent with these findings, using the EW portfolio as a benchmark in pursuit of better out-of-sample performance within the GMV and ERC frameworks. In the next chapter, we outline the techniques used to achieve this aim.


\chapter{Estimation risk reduction techniques} \label{ge_reduce}

As stated in chapter \ref{gen_frame_ch}, estimation risk is comprised of squared bias and variance. There are many methods to approach reducing estimation risk, but in this research,  we introduce three ways that are consistent with the general risk-based investing framework presented in equation (\ref{eqn_general}). The first method deals with improving the estimation of the inputs to the risk-based portfolio function $f(w, \Sigma|\textbf{X})$, accounting for heterogeneity in the input data. The second method involves penalising the optimisation objective function to obtain a portfolio estimate with consistently lower deviation from the actual out-of-sample risk-based portfolio solution. The final process entails changing the implementation method in a manner that reduces estimation risk. Every risk reduction technique falls into one of these three categories.

\section{Improving optimisation inputs}

The first approach to improving on the sample ERC and GMV portfolios involves finding better estimates for the input $\Sigma$, given the set of sample returns. As stated in equation (\ref{ret_mod}), the function $g$ deals with the sample returns in a manner that ensures the irreducible error, $\phi^2$, is independent and identically normally distributed. However, because $g$ is unobservable, our estimation $\hat{g}$ might not ensure this property. Heterogeneity of sample errors for investment portfolios has been observed in empirical finance by \cite{AC02}, who demonstrate that negative stock price correlations are less pronounced in downward markets. Therefore, empirical finance suggests two states of the world, one where the market is in turmoil, and one where the market is not. 

\cite{K12} use these two states to determine separate multi-asset allocations for `turbulent' and `quiet' markets and adopt a regime switching (RS) approach. To define two regimes, a metric to measure turbulence is required. The authors take a squared Mahalanobis distance (SMD) approach to determine an index through time. The Mahalanobis distance is a multi-dimensional generalisation of the notion of how many standard deviations a point is away from the mean of a distribution. The SMD index ($d_t$) is expressed mathematically as:
\begin{align}
d_t = (R_t - \mu)\Sigma^{-1}(R_t - \mu)^\intercal,
\end{align}
where: $$d_t = \mu_{s_t} + \sigma_{s_t}\epsilon_t,$$ and $\epsilon_t$ has a standard normal distribution. The state at time $t$ is shown by the random variable $s_t$. As asserted earlier, the two states are Quiet ($Q$) and Turbulent ($T$), hence $s_t \in \{Q, T\}$. To calculate the SMD, we require the unobservable inputs $\Sigma$ and $\mu$. They can be replaced by their sample counterparts, $\textbf{S}$ and $\hat{\mu}$, to yield $\hat{d}_t$. The SMD has a state-specific mean and volatility; hence different values are observed based on the current system state. If the system emits large values of $\hat{d}_t$, the probability of being in a turbulent market is high. If the system emits small values of $\hat{d}_t$, the likelihood of being in a quiet market is high. The $\zeta^{th}$ quantile of the sample SMDs is the point where the system of reference changes. The market state is also an unobservable variable, so the above model is referred to as a hidden Markov model (HMM). The HMM used in this investigation is depicted in figure \ref{hmm}.

\begin{figure} [h] 
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt, line width=1pt, 
                                  node distance=1cm, style ={minimum size=15mm}]
\tikzstyle{every node}=[font=\footnotesize]
\node [circle, draw] (a) {$T$};
\path  (a) edge [loop above]  node [text width=0cm,midway,left] {$p_{TT}$}(a);
\node [circle, draw] (b) [right=of a] {$Q$};
\path  (b) edge [loop above] node [text width=1cm,midway,right] {$p_{QQ}$}(b);
\draw[->] (a) -- node [text width=0.5cm,midway,above] {$p_{QT}$, $p_{TQ}$}  (b);
\draw [->] (b) -- (a);
\node [circle, draw] (c) [below=of a] {$\hat{d}_t > \hat{d}_{\zeta}$};
\draw[dashed, ->] (a) -- (c);
\draw[dashed, ->] (b) -- (c);
\node [circle, draw] (d) [below=of b] {$\hat{d}_t \leq \hat{d}_{\zeta}$};
\draw[dashed, ->] (a) -- (d);
\draw[dashed,  ->] (b) -- (d);
\end{tikzpicture}
\caption{Turbulent/quiet hidden Markov model.} 
\label{hmm}
\end{figure}

The transition matrix stores the probabilities of transition from a state at time $t$ to another at time $t + 1$ for ease of computation. It is mathematically shown as:
$$P_{t, t + 1} = \begin{bmatrix} 
p_{QQ} & p_{QT} \\
p_{TQ} & p_{TT}  
\end{bmatrix},$$
where the matrix applies to all times $t$. Because the matrix applies to all times the system is called stationary, and the long-run probabilities of being in each state will converge. Once we have determined the most likely state at time $t$ using an algorithm such as the Viterbi algorithm,  we can use the series of estimated states \newline $\{\hat{s}_t: t \in \{0, 1, ..., T\}\}$ to partition the data history. The two datasets would be data used for the quiet sample covariance matrix and data used for the turbulent sample covariance matrix. \cite{FD18} blend these two sample matrices using the investor's risk preferences and the most recent probabilities of being in each state, yielding a more sophisticated estimator of $\Sigma$.

An alternative approach to dealing with heterogeneity is to focus on the assumption that errors for return forecasting models have a normal distribution. But first, we need to define a general return forecasting model. If the return of a portfolio is viewed through a set of return drivers or risk factors, then returns could be explained in part by those factors and the portfolio's sensitivity to them. \cite{M10} encapsulates this idea in his asset return model given below:
\begin{align} \label{ret_fact}
R &= \alpha +\beta^\intercal \mathcal{F} + \epsilon, 
\end{align}
where $R$ is a vector of asset returns (not portfolio returns through time as shown previously), $\alpha$ is a forecastable vector of returns unique to each security, $\beta$ is a matrix of sensitivities to risk factors, $\mathcal{F}$ is a vector of factors, and $\epsilon$ is the error vector. The error vector is assumed to have a normal distribution. \cite{AC02} show that in a downward market, the correlation structure is significantly different from what is implied by a normal distribution, which is a problem when using model (\ref{ret_fact}) in the exhibited way. \cite{CDG19} address the issue of non-normal errors by utilising the quantile regression model proposed by \cite{KB78}. The asset returns, idiosyncratic asset returns, asset factor sensitivities and errors could all be considered to be a function of the current quantile, denoted $\tau$. This leads to the quantile factor model (QFM):
\begin{align} \label{qfm}
\mathcal{Q}(\tau) =  \alpha(\tau) + \beta(\tau)\mathcal{F} + \epsilon(\tau),
\end{align}
where $ \tau \in [0, 1]$. In a different symmetric, normally distributed world, $\tau$ can be set to $0.5$ and model (\ref{ret_fact}) will be recovered. However, in the real world where symmetry and normality are often not adhered to, the quantile conditional errors can be defined more generally so that they only have to satisfy:
\begin{align}
\mathbb{P}\Big[\epsilon(\tau) \leq \underline{0} \, \Big | \mathcal{F} \Big] = \tau \; .
\end{align}
This structure emerges from the cumulative distribution function (CDF) conditional on the set of factors of each asset return $R_i$. Given the conditional CDF for the returns on asset $i$, $F_i(R_i|\mathcal{F})$, the quantile specific inverse CDF, $F^{-1}_i(\tau| \mathcal{F})$, can be used to generate the quantiles $\mathcal{Q}_i(\tau)$. \cite{FD18} suggest using the information about each quantile to construct a series of quantile-specific covariance matrices, which can then be blended to yield a more sophisticated estimator of $\Sigma$. Chapter \ref{some_maths} covers the implementations of these two techniques to better estimate $\Sigma$.

\section{Penalising the optimization}

To add a penalty term in a way that preserves the goal of the risk optimisation, we first need to adapt the objective functions of each risk-based portfolio as given earlier in chapter \ref{rb_ports}. Consider the return-targeting penalised optimisation approach of \cite{K18}, both choices of $f(\cdot|\textbf{X})$ for the GMV and ERC portfolios can be adapted into this approach. Beginning with the GMV portfolio, Kinn views the portfolio variance as an expectation:
\begin{align*}
f(w, \Sigma  | \textbf{X}) &= w^\intercal \Sigma w &\\
& = w^\intercal (\mathbb{E}[{r}_t {r}_t^\intercal] - \mu \mu^\intercal) w & \small\text{(alternate definition of $\Sigma$)} \\
& = \mathbb{E}[|w^\intercal \mu - w^\intercal {r}_t|^2],
\end{align*}
where ${r}_t$ represents the asset returns above the risk-free rate, and $\mu$ is a vector of the population expected excess returns as before. Rewriting the portfolio expected excess return as $\bar{r} = w^\intercal \mu$, the idea of return-targeting for a portfolio can be incorporated as the expectation of $|\bar{r} - w^\intercal {r}_t|^2$, which is the squared distance to a target return level. Kinn's approach is consistent with an MVO optimisation intuitively because the target return level is analogous an expected return constraint and minimising the return's squared distance to this constraint is analogous to variance minimisation. The objective function can now be approximated using the sample average as a result of the law of large numbers. We still have to show how to find the GMV portfolio from an MVO procedure. As stated in table \ref{tab_rb_port}, the GMV portfolio is the MSR portfolio if the assumption of identical excess returns is met. Therefore, if the target return is set to a value that is easily obtainable $\bar{r} = \bar{r}_\mathrm{gmv}$, then the scheme will yield a GMV portfolio. This easily obtainable value has to be found numerically and cannot be determined a priori. The non-rigorous argument turns out to be empirically true for the portfolios analysed in this research. When the return vector is replaced by the set of sample returns $\textbf{X}$, and the expectation is approximated by the sample average, the \cite{K18} form of objective function is recovered:
\begin{align} \label{kinn_op_1}
f_{\text{Kinn}}(w|\textbf{X}) &= \frac{1}{T} \sum_{t = 1}^T  (\bar{r}_\mathrm{gmv} - \textbf{X}^\intercal_t w)^2,
\end{align}
where $t$ indexes columns of the sample returns matrix $\textbf{X}$. The GMV portfolio can be found equivalently in this way. 

The log-constraint\footnote{The log-constraint is also introduced in appendix \ref{vol_ineq}.}, $\sum_i \ln(w_i) \geq c$, can be placed on optimisation (\ref{kinn_op_1}) to recover the ERC portfolio. To include the constraint in framework (\ref{eqn_general}), a Lagrangian multiplier approach can be used to move the log-constraint to the objective function. Kinn's adapted ERC objective function is then:
\begin{align}
f_{\text{Kinn}}(w|\textbf{X}) &= \frac{1}{T} \sum_{t = 1}^T  (\bar{r}_{gmv} - \textbf{X}^\intercal_t w)^2 -\eta_{erc}\sum_{i = 1}^N \ln(w_i),
\end{align}
where $\eta_{erc}$ is the Lagrangian multiplier scalar. Now that we have shown the standard objective functions from earlier are equivalent to the Kinn framework, we have also vindicated the general framework (\ref{eqn_general}) as accommodative of a valid application of supervised machine learning (SML) to portfolio optimisation.

Because logical choices for $f_{\text{Kinn}}(\cdot|\textbf{X})$ have been established, different penalty functions can be applied to the optimisation. Two common penalised regression techniques are lasso regression and ridge regression (RR). In the presence of a long-only constraint, as is applied in this research, a lasso regression does not make sense, because the penalty function is simply the sum of the absolute weights: $P(w) = \sum_{i=  1}^N |w_i|  =1$. This penalty is equal to $1$ for all constrained portfolios. Separately, the RR is obtained by specifying the penalty as the sum of squares for the portfolio weights: $P(w) = \sum_{i=  1}^N w_i^2$. The penalty reduces the number of admissable concentrated portfolios and intuitively is not unlike incorporating some of the EW portfolio into the ERC or GMV portfolios. Ignoring constraints, \cite{LW04} show that RR has the same effect as shrinking the sample covariance matrix towards the identity matrix for the GMV optimisation:
\begin{align}
\textbf{S}_{RR} = \textbf{S} + \frac{\lambda}{T} \textbf{I}, 
\end{align}
where $\lambda$ is the shrinkage intensity, and $T$ is the number of sample observations. In the presence of constraints, the actual scaling factor is slightly different from Ledoit and Wolf's calculations, but the intuition of shrinkage towards the identity matrix still applies. If $\lambda$ becomes very large, the minimum variance portfolio will tend towards the EW portfolio. Estimating lambda is thus a practical choice, and the process to do so consistently is outlined in the next chapter.

\section{Alternate implementation methods}

\cite{SW17} present a means to find a resampled MVO portfolio that reduces estimation risk by optimising random subsets of assets in the investment universe. The process is called subset resampling (SRS). They then aggregate resultant optimised subset portfolios to create a final `optimal' solution. The procedure requires the inputs of a sample return matrix $\textbf{X}$ and an asset subset size $b$. The subset size is related to the extent of the trade-off between bias and variance. We have to choose the degree of repeated sampling, $s$, which is restricted by the available computational power.

This method can be described as follows. For each of the $s$ repeated samples, we randomly select the $j^{\text{th}}$ subset of $b$ assets from the $N$ assets in the investment universe, denoted $\mathcal{I}_j$. Using only the sample return data from the selected asset subset $\textbf{X}_j$, we then compute the associated optimal portfolios $\hat{w}_j$ using framework (\ref{eqn_general}) and a given choice of objective function. Finally, we average the $s$ optimal subset weight vectors to obtain the final optimal asset portfolio $\hat{w}_\mathrm{srs} = (\sum_{j = 1}^s \hat{w}_j)s^{-1}$.

The SRS process is very general, and could even be applied in conjunction with a penalised optimisation or an improved sample covariance matrix. Additionally, the user can choose the input $b$. If $b = N$, then the usual sample risk-based portfolio is recovered, albeit in a computationally expensive manner. If $b = 1$, then the SRS procedure will yield the EW portfolio for a large enough value of $s$. Therefore, $b$ is the input parameter controlling the extent of the trade-off between weight diversification and estimation risk. The estimation of $b$ should be done in a manner consistent with the aim of the optimisation. To ensure $b$ scales with the size of the asset universe, \cite{SW17} recommend writing it in the form $b = N^{\alpha}$, where $\alpha \in [0, 1]$.

The SRS method is comparable to ensemble methods in machine learning. The logical basis is that many different models can be used and aggregated into a final model, rather than assuming a single model is the most accurate to use. Despite the general nature of the SRS procedure, it is still consistent with the approach of increasing the squared bias out of the hope that the variance reduction will offset it enough to lessen overall estimation risk.

Each of the three estimation risk reduction classes has at least one specific technique within them. Some modelling decisions need to be made for a prospective user to apply these techniques in an experiment. These modelling decisions are covered in the next chapter.

\chapter{Using estimation risk reduction techniques} \label{some_maths}

Under the EW and MW portfolios, the portfolio weights do not require estimation whereas the GMV and ERC portfolios do. For each of these portfolios, there are six given ways of performing the estimation in this research. The first is by using the standard sample covariance matrix, the next four are to use techniques outlined in chapter \ref{ge_reduce}, and the final one is to use a combination of quantile factor modelling and regime switching. A summary of all of the portfolio technique pairs is given in table \ref{tab_pairs} below.

\begin{table} [h]
\footnotesize
\caption{Portfolio-technique pairs.}
\centering
 \begin{tabular}{p{5cm}p{5cm}p{2.5cm}} 
 \hline
\textbf{Portfolio} & \textbf{Technique} &  \textbf{Abbreviation}\\ 
\hline
Equal weight & - & EW \\
Market weight  & -&  MW \\
\\
Global minimum variance&Sample covariance matrix&GMV-SCM \\
Global minimum variance&Quantile factor modelling & GMV-QFM \\
Global minimum variance&Regime switching & GMV-RS \\
Global minimum variance&Quantile factor modelling with regime switching & GMV-QRS \\
Global minimum variance&Ridge regression & GMV-RR \\
Global minimum variance&Subset re-sampling & GMV-SRS \\
\\
Equal risk contribution&Sample covariance matrix&ERC-SCM \\
Equal risk contribution&Quantile factor modelling &ERC-QFM \\
Equal risk contribution&Regime switching & ERC-RS \\
Equal risk contribution&Quantile factor modelling with regime switching & ERC-QRS \\
Equal risk contribution&Ridge regression & ERC-RR \\
Equal risk contribution&Subset re-sampling & ERC-SRS \\
\\
 \hline
\end{tabular}
\label{tab_pairs}
\end{table}

\section{Quantile factor modelling and regime switching}

QFM and RS improve the estimation of the inputs into the optimisation and therefore have the same implementation for the GMV and ERC portfolios. To use the RS model for covariance prediction, the probability of being in either a quiet or a turbulent state needs to be estimated. If $s_t$ is the random variable that takes on the value of the state at time $t$ then $s_t \in \{Q, T\}$. The true parameters are defined as:
\begin{align*}
\pi_{Q, t} &:= \mathbb{P}\{s_t = Q\}, \\
\pi_{T, t} &:= \mathbb{P}\{s_t = T\} = 1 - \pi_{Q, t}.
\end{align*} 
These can be estimated using the emission quantities from the HMM, $\hat{d}_t$, and a maximum likelihood approach - the Baum-Welch algorithm\footnote{An influential tutorial on this algorithm and HMMs was given by \cite{R89}.}.  \cite{K12} determine a turbulent-quiet data split through the parameter $\zeta$, which is the proportion of the data allocated to the quiet regime for covariance estimation. They suggest a value of $0.7$ or $0.8$, but due to constraints on input data length we use $0.7$ in this research. The investor has nearly estimated enough parameters to blend the variance-covariance matrix using the method of \cite{FD18} outlined below in equation (\ref{fd_blend}).
\begin{align} \label{fd_blend}
\hat{\Sigma}_{\text{blend}} & = \hat{\pi}_{Q, t + 1} \hat{\eta}_Q \hat{\Sigma}_Q + \hat{\pi}_{T, t + 1} \hat{\eta}_T \hat{\Sigma}_T.
\end{align}
The estimated probabilities,  $\hat{\pi}_{Q, t + 1}$ and $\hat{\pi}_{T, t + 1}$, are forward-looking. Predictions can be made empirically using the current state because the transition probabilities are generally near $1$ or $0$ in the data. The investor still has to estimate their normalised aversion to each regime $\hat{\eta}_{s_{t+1}}$, which they can do using the estimation procedure from \cite{B18}. For the experiment, the investor was indifferent between regimes.

The intention of this blending procedure is for money managers to estimate the state probabilities themselves, thus incorporating their future beliefs. In reality, when implementing a quantitative model, it is common to use a rolling estimation window of data. Asset return data is often not long enough to accommodate sophisticated portfolio construction methods. Therefore, assigning $\hat{\pi}_{T, t + 1}$ values near $1$ results in the estimation and application of large covariance matrices with potentially only $30 \%$ of the data, without the requisite weighting of the other covariance matrix. The inaccurate covariance matrix leads to the very same input sensitivity problems that the RS model attempts to avoid. It is worth clarifying that the sensitivity is due to an incorrectly estimated $\hat{\pi}_{T, t + 1}$, not the absence of underlying regimes. To correct this sensitivity concern, a discretised simplification is used. If the state is estimated to be quiet, nothing is done to the weights implied by the volume of data:
\begin{align*}
(\hat{\pi}_{Q, t + 1}, \hat{\pi}_{T, t + 1}) = (\zeta, 1 - \zeta).
\end{align*}
If the state is estimated to be turbulent, then the weights implied by the volume of data are adjusted to overweight the turbulent regime by a proportion of $\gamma$:

\begin{align*}
(\hat{\pi}_{Q, t + 1}, \hat{\pi}_{T, t + 1}) = (\zeta -  \frac{\gamma}{2}, 1 - \zeta + \frac{\gamma}{2}).
\end{align*}
In the experiment, we set $\gamma$ to 0.1. The effect is that in the quiet regime, the covariance matrix is the same as without the RS model. While during the turbulent regime, the turbulent covariance matrix is given a weighting of $\gamma$ more than what is implied by the data split.

The QFM technique is examined next. \cite{CDG19} use the QFM technique from equation (\ref{qfm}) for prediction. They select risk factors and estimate factor loadings using a simultaneous procedure. Factor-based modelling is not the focus of this research, although it can be used in conjunction with the general framework (\ref{eqn_general}). In this experiment, mainly for pedagogical purposes, simple factors are used for the QFM portfolios; namely, a market factor and a squared market factor. This choice is consistent with the findings of \cite{TM66}, with more detail given in appendix \ref{qfm_e}. Additionally, the QFM user has to choose how to partition the interval $[0, 1]$, i.e. decide which quantiles to use. The partition is an important input which could be estimated. We use the same split as \cite{MP08}, where: $$[\tau_0 = 0, \tau_1 = 0.1]\cup(\tau_1 = 0.1, \tau_2 = 0.9] \cup (\tau_2 = 0.9, \tau_3 = 1].$$  In the notation, the intervals are referred to by their right endpoint. \cite{FD18} note that the QFM, as stated, does not provide variation between quantiles, as the idiosyncratic error term will always adjust so that the total estimated covariance matrix is always equal to the sample covariance matrix. Therefore, to estimate the quantile specific covariance matrices, they propose fixing the error term to the median error such that $\hat{\epsilon}(\tau) = \hat{\epsilon}(0.5)$. Then each set of quantile returns can be used to construct a sample covariance matrix denoted $\hat{\Sigma}^{(\tau)}$. \cite{MP08} propose a strategy for portfolio forecasting called quantile regression portfolio distribution (QRPD). It involves interval-weighting the portfolio allocations $\hat{w}^{(\tau)}$ for each quantile covariance matrix. The QRPD portfolio is:
\begin{align}
\hat{w}_{\text{QRPD}} & = \sum_{i = 1}^{l} p_i\hat{w}^{(\tau_i)},\;\;
\end{align}
where $p_i = \tau_i - \tau_{i - 1}$, and $l$ is the number of intervals. Like the SRS method, the QRPD method is comparable to ensemble methods because different models are being aggregated in the hope of reducing total estimation risk.

The QRS technique from table \ref{tab_pairs} is laid out by \cite{FD18}. They first separate the data history into regimes. Within each regime, they apply the QFM approach. Once they find $\hat{\Sigma}_{s_t}^{(\tau_i)}$ for every quantile and state, they blend the covariance matrices between regimes. For every possible pair of quantiles $\tau_i, \tau_j \in \{\tau_1, ..., \tau_{|\tau|}\}$, a blended SCM can be constructed:
\begin{align}
\hat{\Sigma}^{(\tau_i, \tau_j)}_{\text{blend}} & = (\hat{\pi}_{Q, t  + 1}\hat{\eta}_Q\hat{\Sigma}^{(\tau_i)}_Q) + (\hat{\pi}_{T, t  + 1}\hat{\eta}_T\hat{\Sigma}^{(\tau_j)}_T).
\end{align}
Each of the SCMs can be used to find a portfolio $\hat{w}^{(\tau_i, \tau_j)}$. Once the $l \times l$ portfolios have been found they can also be blended with the QRPD method:
\begin{align}
\hat{w}_{\text{QRPD}} & =  \sum_{i = 1}^{|\tau|}  \sum_{j = 1}^{|\tau|} p_i p_j \hat{w}^{(\tau_i, \tau_j)}.
\end{align}

\section{Ridge regression}

As stated previously, the QFM and RS techniques do not require separate workarounds for the ERC and GMV portfolios. This is not the case for the RR method. First, we consider the GMV portfolio estimation and then the ERC portfolio estimation. The Lagrangian multiplier, $\lambda$, for the penalty in framework (\ref{eqn_general}), should be chosen to minimise the estimation risk. However, the estimation risk is unobservable. \cite{K18} suggests the k-folds cross-validation estimation procedure to select this parameter\footnote{The procedure is outlined by \cite{FHT01}.}. The broad idea of this procedure is to approximate the estimation risk and choose a value of lambda, $\lambda^*$, that minimises the approximated estimation risk. To perform k-folds cross-validation, we initially arrange the observations into $K$ subsets without replacement. The subsets are taken over time and not assets as with SRS technique. The set of observations for the $k^{\text{th}}$ subset is denoted $\mathcal{I}_k$. All of the observations in the $K - 1$ remaining subsets (i.e. excluding those in the $k^{\text{th}}$ subset) are stored in a set denoted $\mathcal{I}_{-k}$. This yields the optimisation:
\begin{align}  \label{score_func}
\lambda^* & = \underset{\lambda}{\text{argmin}} \Big \{ \frac{1}{K} \sum_{k =1}^K \hat{h}_k(\lambda, \mathcal{I}_k, \mathcal{I}_{-k})\Big \},
\end{align}
where $\hat{h}_k(\lambda)$ is a function that approximates estimation risk given a value of $\lambda$. One choice for $h$ is the mean squared error penalty for a portfolio found using the data from $\mathcal{I}_{-k}$ and then tested on $\mathcal{I}_k$. This procedure is described below:
\begin{enumerate}
\item For each subset $k = \{1, 2, ..., K\}$, find the optimal portfolio weights using the training data $\mathcal{I}_{-k}$ and apply the penalty scaled by $\lambda$.
\item Evaluate the performance of this portfolio on the unused data $\mathcal{I}_k$ with the mean squared error loss function and retain the score $\hat{h}_k$:
\begin{align} \label{h_func}
\hat{h}_k (\lambda, \mathcal{I}_k, \mathcal{I}_{-k}) = \frac{1}{|\mathcal{I}_k|} \sum_{i \in \mathcal{I}_k} (\bar{r}_{gmv} - \textbf{X}_i^\intercal \hat{w}_{\mathcal{I}_{-k}}(\lambda))^2,
\end{align}

where $\bar{r}_{gmv}$ is determined as in chapter \ref{ge_reduce}, and the function $|\cdot|$ counts the number of observations in a set.

\item The range of possible $\lambda$ values can be discretised to find a solution that minimises the objective function in (\ref{score_func}) \footnote{The objective function in (\ref{score_func})  can be computationally expensive to evaluate; hence, an efficient approximation procedure is necessary.}.
\end{enumerate}

For the ERC portfolio implementation of the RR method, there is an additional hyperparameter, $\eta_{erc}$, that needs to be estimated. This hyperparameter should be found before the penalty hyperparameter $\lambda^*$. We also use the k-folds cross-validation technique to find this parameter. However, the mean squared error loss to the desired global minimum variance portfolio from equation (\ref{h_func}) is not appropriate for optimising an ERC portfolio. Ideally, $\eta_{erc}^*$ should minimise the distance between all of the total risk contributions so that they are as equal as possible. Therefore, we make use of the following hyperparameter selection function (see appendix \ref{eta_est} for motivation):
\begin{align}
\hat{h}_k^{erc} (\eta, \mathcal{I}_k, \mathcal{I}_{-k}) & = \sum_{i = 1}^N  \sum_{j \geq i}^N(\hat{w}_{i, \mathcal{I}_{-k}}(\textbf{S}_{\mathcal{I}_k} \hat{w}_{\mathcal{I}_{-k}})_i - \hat{w}_{j, \mathcal{I}_{-k}}(\textbf{S}_{\mathcal{I}_k} \hat{w}_{\mathcal{I}_{-k}})_j )^2.
\end{align}


\chapter{An empirical test using South African equities} \label{experiment}

In this chapter, we test the various approaches outlined above for constructing risk-based portfolios using South African equity data. There are two sources of variation in this experiment: variation in risk-based portfolio type and variation in estimation risk reduction technique, all combinations of which are outlined in table \ref{tab_pairs}. Each combination of portfolio-technique approach will be referred to as a pair.

\section{Data and methodology}

The available data for this experiment are weekly total-returns for all equity stocks included in the Johannesburg Stock Exchange (JSE) All Share Index (ALSI) over the period ranging from the $5^{th}$ January 1996 to the $1^{st}$ November 2019. Weightings of the shares in the ALSI are also available, although this is a monthly series. This historical vector of the weights of the MW portfolio through time is denoted $\hat{w}^{\text{mw}}_t$.

The investigation uses a rolling estimation window of $T = 400$\footnote{$400$ weeks or $7.69$ years may seem like an odd choice. It is a window that balances the need for enough estimation data, but also a lengthy out-of-sample test. For a longer data history, a larger estimation window could be used.} weekly observations to construct portfolios. Weekly data inputs are used with monthly rebalancing because of the high data volume requirement from chapter \ref{some_maths}. The constructed portfolio $\hat{w}_{t_i}$ will be held out-of-sample over the interval $[t_i, t_{i + 1}]$, where $t_i$ denotes a month's end. The entire holding period is from the $30^{th}$ of September 2003 to the $27^{th}$ of September 2019 a total of 16 years. Portfolios are constructed for two different asset universes; namely, the $40$ largest stocks and the $100$ largest stocks. The separation is to test the effect on portfolio performance of an investor broadening their universe. This methodology is outlined for each rebalancing date on the next page: 
\begin{enumerate}
\item Of the stocks in the ALSI with a data history of at least length $T$,  choose the $N$ stocks with the largest market weights.
\item Use the $N  \times T$ matrix of sample returns to find asset weights for the relevant portfolio-technique pair.
\item Hold the found portfolio $\hat{w}$ for one month, record the returns $\hat{r}$, and calculate the portfolio turnover.
\end{enumerate}

This experiment is specific to this data and method. Therefore, it is important to outline some limitations of the results. Although framework (\ref{eqn_general}) is broad, this experiment only analyses portfolios constructed with South African equity returns, and cannot be used to state facts about the portfolio construction process generally. The budget, long-only and maximum weight constraints were all applied for this experiment. Specifically the maximum weight constraint used is $\alpha = 0.1$. The choice intends to provide a consistent basis for comparison across portfolio-technique pairs and ensures similarity to practical applications. Shares that have missing data were excluded at each rebalancing date with a filter, therefore the MW portfolio, where $N = 40$ may not be representative of the JSE Top 40 index. Transaction costs are ignored due to the liquid nature of the largest stocks, although this is a potential area for future improvement on the backtesting methodology.

\section{Results}

Before the results are reported, specific metrics that illustrate the effect of each technique and portfolio need to be introduced. The first is turnover (TO). It measures the magnitude of trading required on each rebalancing date. If there are $m$ rebalancing dates, then the TO is calculated as:
\begin{align}
\text{TO} & = \frac{6}{(m - 1)} \sum_{i = 1}^{m - 1} \sum_{j =1}^{N} |\hat{w}_{t_i, j}^\triangle - \hat{w}_{t_i, j}|,
\end{align}
where $\hat{w}_{t_i, j}^\triangle$ is the buy-and-hold weight of the $j^{th}$ asset just before the rebalance at time $t_i$. The turnover is annual and one-way only; hence, the scaling factor of $\frac{6}{m-1}$. For a higher TO, the investor has increased risk when rebalancing that they will not enter into the new portfolio at the current market price. The maximum possible value of the TO is $1200\%$, and the investor achieves it if they switch from one single stock portfolio to another at every rebalancing date.

The next reporting metric is maximum drawdown (MDD), the definition of which requires the notion of cumulative wealth. The investor's cumulative wealth at time $t_j$ is their return from time $t_0$ until time $t_j$ on a portfolio of one initial unit investment, and it is defined as:
\begin{align*}
W_{t_j} &= \prod_{i = 1}^{j} (1 + \hat{r}_{t_i}), 
\end{align*}
where $\hat{r}_{t_i}$ is the return realised over the interval $[t_{i - 1}, t_i)$. The MDD is the biggest loss in cumulative wealth for the entire investment period, and is formulated as:
\begin{align}
\text{MDD} & = \underset{t_j \in \{t_1, ..., t_m\}}{\text{argmax}} \Big \{ \underset{t_i \in \{t_1, ..., t_j\}}{\text{argmax} \{W_{t_i} \}} - W_{t_j}  \Big \} \; .
\end{align} 
MDD is an essential measure for money managers because excessive drawdowns lead to redemptions in their funds. \citep{M04}

The last two measures are measures of concentration. As stated earlier, the risk-based investor is making a trade-off between weight concentration and risk concentration. A standard for weight concentration at a rebalancing date, the inverse Herfindahl index, has already been defined. But the IHI is general and could apply to multiple types of weights. For allocation weights the IHI at time $t_i$ is given as:
\begin{align}
N^{\text{eff}}_{t_i} &= \frac{1}{\sum_{j = 1}^N w_{t_i, j}^2} \; ,
\end{align}
where $N^{\text{eff}}_{t_i} $ can be interpreted as the number of equal-weighted stocks the investor's portfolio is equivalent to. The notation reflects the idea that the IHI represents the number of effective stocks in the portfolio. The upper bound of $N^{\text{eff}}_{t_i} $ is $N$ and the lower bound in the presence of a maximum allocation constraint is: $ ({\lfloor \frac{1}{\alpha} \rfloor \cdot \alpha^2 + (1- \lfloor \frac{1}{\alpha}\rfloor \cdot \alpha)^2})^{-1}$, which is  derived in appendix \ref{IHI_max}. For the case when $\alpha = 0.1$ the lower bound is $10$. Risk-weights can be taken instead of capital weights to measure the effectiveness of an asset on portfolio volatility. Since the TRCs sum to the portfolio volatility, the risk contributions can be scaled into risk-weights so that they sum to $1$. The risk-weight for the $j^{th}$ asset at time $t_i$ is given as:
\begin{align}
\text{RW}_{t_i, j} &= \frac{\text{TRC}_{t_i, j}}{\sqrt{w_{t_i}^\intercal \Sigma w_{t_i}}}\; ,
 \end{align}
where the inverse Herfindahl index approach can be applied again. The risk-weight IHI is written as:
\begin{align}
N^\mathrm{rw}_{t_i}(RW_{t_i}) &= \frac{1}{\sum_{j = 1}^N \text{RW}_{t_i, j}^2} \;.
\end{align}
If $\Sigma$ is positive semi-definite, then the lower bound is $1$ even in the presence of the maximum allocation constraint, because one asset could have a positive weight and considerable volatility. The upper bound is once again $N$. The concentration measures are reported as averages across all of the $m$ rebalancing dates.

Let us first discuss the results for the GMV portfolios. The main objective of constructing GMV portfolios is to reduce the out-of-sample volatility of returns. The main aim of each GMV-technique pair is the same, but performance is measured against the GMV-SCM pair. Therefore, it makes sense to define a measure of each GMV-technique pair performance in the same way as \cite{RR15}, as a volatility reduction over the GMV-SCM pair:
\begin{align}
\mathcal{VR}(w|w_{\text{gmv-scm}}) & = \frac{\sigma(w_{\text{gmv-scm}}) - \sigma(w)}{\sigma(w_{\text{gmv-scm}})},
\end{align}
where the $\sigma(\cdot)$ function measures a portfolio's volatility. In figures \ref{GMV_comp_40} and \ref{GMV_comp_100} the 3-year rolling volatility reduction profiles for each of the different techniques can be found\footnote{A 3-year window ensures more volatility stability through time so that comparisons are more meaningful.}. We excluded the EW and MW portfolios for relative scaling. The RR and SRS techniques result in portfolios that have different volatility properties through time when compared to the SCM, QFM and RS techniques. For a rolling window that includes the 2008 global financial crisis, the SCM outperforms the RR and SRS techniques. 

\begin{figure}[H]
\includegraphics[width=14.5cm]{n40_3_yr_roll_vol.pdf}
\caption{3-year rolling annualised portfolio volatility reduction as a percentage of the SCM-GMV portfolio for the 40-stock universe.}
\label{GMV_comp_40}
\end{figure}
\begin{figure}[H]
\includegraphics[width=14.5cm]{n100_3_yr_roll_vol.pdf}
\caption{3-year rolling annualised portfolio volatility reduction as a percentage of the SCM-GMV portfolio for the 100-stock universe.}
\label{GMV_comp_100}
\end{figure}

This result is not consistent with other studies that show improved performance for various shrinkage techniques over the SCM, see \cite{GF14} for examples. However, other studies often use shorter estimation windows than $7.69$ years, which are suited to covariance matrix shrinkage. 

The SCM outperformance trend is reversed once the tumultuous financial crisis period falls out of the rolling window. The global financial crisis is the specific event through which it would be desirable for the portfolio volatility to remain low for the risk-averse investor, which makes this empirical observation a strong affirmation of the SCM technique. The evidence is consistent with \cite{K10}, who also consider a longer estimation window. They attribute the SCM outperformance to an over-reliance on short estimation windows that lead to portfolios allocations that would be rejected by a prudent investor. The QFM and RS techniques both report an improvement on the GMV-SCM portfolio in terms of out-of-sample volatility, but the level of volatility reduction is small. For the portfolio of $40$ stocks, the volatility reduction is consistently between the levels of 0.5\% and 2\%.

Table \ref{test_perform} outlines the backtested out-of-sample performance for all of the portfolio-technique pairs. The MW and EW portfolios are shown for comparison. The performance numbers are aggregated over the entire sample period. Continuing with the comparison of GMV techniques, the GMV-SCM portfolio's MDD is almost $3\%$ lower than the GMV-RR portfolio. Empirically this suggests that a squared weights penalty is not suited to GMV portfolios which specifically aim for weight concentration in the least volatile stocks.

The similar aggregated volatility reduction performance of the RR and SRS techniques, and the fact that they both increase the number of effective stocks in the out-of-sample portfolio suggests that in the experiment (i.e. for South African equities only) they have similar effects. More specifically, they increase $N_{\text{eff}}$ for 40-stock portfolios from 13.29 for the SCM technique to 26.23 and 21.93 for the RR and SRS. This trend is also observed for 100-stock portfolios and is the effect outlined theoretically as shrinkage towards the EW portfolio.
\begin{table} [H]  \footnotesize
\caption{Portfolio-approach pair performance.}
\centering
\label{test_perform}
\begin{tabular}{p{1.3cm}p{1.4cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}} 
\hline 
\textbf{Portfolio }& \textbf{Metrics} &\textit{SCM}&\textit{QFM}&\textit{RS}&\textit{QRS}&\textit{RR}& \textit{SRS} \\
\hline
& &&&&&&\\
GMV   & $\hat{\sigma}_{p.a.}$  [\%] & 14.08 & 14.03 & 14.07 & 13.99 & 14.36 & 14.15\\
(N = 40) & $N_{\text{eff}}$&13.29 & 13.96 & 13.23 & 14.05 & 26.23 & 21.93\\
& $N_{\text{rw}}$&31.73 & 32.05 & 31.91 & 32.57 & 37.44 & 35.82\\
& SR &0.78 & 0.79 & 0.78 & 0.81 & 0.75 & 0.80  \\
&   TO [\%] &94.23 & 95.99 & 103.48 & 107.24 & 80.19 & 74.20\\
&  MDD [\%] & 28.77 & 28.89 & 28.55 & 28.34 & 31.52 & 29.58\\
&\\
GMV   & $\hat{\sigma}_{p.a.}$ [\%]&12.18 & 12.16 & 12.22 & 12.20 & 12.30 & 12.23\\
(N = 100) & $N_{\text{eff}}$&19.41 & 20.41 & 18.95 & 20.22 & 48.83 & 33.67\\
& $N_{\text{rw}}$&77.01 & 77.20 & 77.63 & 79.11 & 91.07 & 85.10\\
& SR &0.80 & 0.82 & 0.80 & 0.82 & 0.79 & 0.80\\
&   TO [\%]&100.97 & 102.33 & 117.16 & 119.49 & 67.08 & 76.88\\
&  MDD [\%]&30.27 & 30.14 & 30.51 & 30.27 & 31.75 & 30.78 \\
&\\
ERC   & $\hat{\sigma}_{p.a.}$ [\%]&15.07 & 15.04 & 15.02 & 15.04 & 15.03 & 14.91\\
(N = 40) & $N_{\text{eff}}$&36.63 & 36.49 & 36.25 & 36.27 & 36.13 & 34.57\\
& $N_{\text{rw}}$&39.26 & 39.25 & 39.24 & 39.23 & 39.22 & 38.96\\
& SR &0.69 & 0.69 & 0.69 & 0.69 & 0.70 & 0.71\\
&   TO [\%] &53.10 & 54.79 & 56.23 & 57.25 & 54.20 & 58.90\\
&  MDD [\%]&32.86 & 32.68 & 32.76 & 32.89 & 32.74 & 32.22\\
&\\
ERC   & $\hat{\sigma}_{p.a.}$ [\%]&13.56 & 13.56 & 13.54 & 13.43 & 13.87 & 13.26\\
(N = 100) & $N_{\text{eff}}$&84.75 & 84.93 & 83.18 & 77.73 & 85.02 & 76.21\\
& $N_{\text{rw}}$&96.31 & 96.37 & 96.37 & 96.05 & 96.36 & 96.11\\
& SR &0.71 & 0.69 & 0.69 & 0.67 & 0.64 & 0.69\\
&   TO [\%] &80.78 & 84.45 & 86.61 & 99.63 & 135.71 & 116.55\\
&  MDD [\%]&33.57 & 34.36 & 34.37 & 33.43 & 34.58 & 32.86\\
\end{tabular}
\begin{tabular}{p{13.6cm}}
\hdashrule[0.5ex][c]{\linewidth}{0.5pt}{1.5mm}
\end{tabular}

\begin{tabular}{p{1.3cm}p{1.4cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.2cm}} 
\textbf{Portfolio }& \textbf{Metrics} & \textit{N = 40}& \textit{N = 100}& \textbf{Portfolio }& \textbf{Metrics} & \textit{N = 40}&  \textit{N = 100} \\
\hline
&&&&&& \\
EW   & $\hat{\sigma}_{p.a.}$  [\%] & 15.79 &14.53& MW   & $\hat{\sigma}_{p.a.}$  [\%] & 17.99 & 17.36\\
 & $N_{\text{eff}}$&40&100&  & $N_{\text{eff}}$&14.02& 17.28\\
& $N_{\text{rw}}$& 39.22  &96.05&& $N_{\text{rw}}$&29.46 & 57.24 \\
& SR & 0.63 &0.63&&  SR &0.54& 0.55 \\
&   TO [\%] & 45.44 &42.56&&   TO [\%] &41.75 & 40.96 \\
&  MDD [\%] & 34.60 &36.94&&  MDD [\%] & 44.01 & 42.83\\
\\
\hline
\end{tabular}

\end{table}

For ERC portfolios, the main objective is to equalise the out-of-sample risk contributions, which is the same as maximising the number of effective risk-weights. Observing $N_{\text{rw}}$ from table \ref{test_perform} over the out-of-sample period, it appears that the SCM technique has performed well, ranking as the best for the $40$-stock portfolio and only narrowly being behind the QFM, RS and RR methods for the $100$-stock portfolio. 

The effective risk-weights require a long out-of-sample window to observe because $\Sigma$ has to be estimated to find risk-weights. Therefore, as an alternate approach, the rolling 3-year Sharpe ratio is used. Because the SR can go negative, we observe the difference with the SCM as opposed to a ratio. These time series are plotted in figures \ref{ERC_comp_40} and \ref{ERC_comp_100} for the 40 and 100-stock portfolios respectively. Once again, it is clear that even with the alternate measure of performance, the SCM performs adequately, together with the SRS technique. The slightly lower Sharpe ratios through time for the SCM technique 40-stock portfolios are small in magnitude in comparison to the 100-stock portfolios. Generally SRS perform well and \cite{SW17} report a similar type of superiority for the technique in MVO portfolios for the US equity universe. Overall, accounting for risk-weight diversification, the SCM technique appears to have performed the best, whereas for GMV portfolios the RS and QFM techniques outperformed.

\begin{figure}[H]
\includegraphics[width=14.5cm]{n40_3yr_roll_sr.pdf}
\caption{3-year rolling annualised portfolio Sharpe ratio difference with the SCM-ERC portfolio for the 40-stock universe.}
\label{ERC_comp_40}
\end{figure}
\begin{figure}[H]
\includegraphics[width=14.5cm]{n100_3yr_roll_sr.pdf}
\caption{3-year rolling annualised portfolio Sharpe ratio difference with the SCM-ERC portfolio for the 100-stock universe.}
\label{ERC_comp_100}
\end{figure}

Returning to table \ref{test_perform} and comparing the 40-stock and 100-stock results, it is clear that expanding the asset universe for GMV portfolios leads to reduced out-of-sample volatility (approximately 2\%). This volatility reduction between asset universes of 14\% is more significant than the observed decrease between any two techniques. Therefore, a South African equity investor that always contributes the smallest marginal risk amount to their portfolio at the margin would be advised to broaden their investment horizon. This effect is less pronounced for ERC, EW and MW portfolios.

% \textit{You can also actually calculate the break-even TC's required in order to make zero a given portfolio's outperformance relative to another. I.e. if a portfolio with 1000\% turnover per annum outperforms another with 0 \% turnover by 2 \%, then the transaction costs need to be at least 20bps to zero that outperformance number (2 \% / 10).  So calculate break-even TC ex-post is actually really easy (but must be done relative to another portfolio).}

Relative to the unoptimised benchmark portfolio (MW portfolio) all 26 risk-based portfolio-approach pairs from table \ref{test_perform} outperformed on a Sharpe ratio measure. This is an empirical vindication for investing with risk-based frameworks in South Africa. For the EW portfolio, the relative outperformance of the MW portfolio is consistent with the findings of \cite{DGL07}, but for South African equities. Within risk-based portfolios, the sample volatilities still adhere to the volatility inequality across all techniques: $\hat{\sigma}(\hat{w}_{gmv}) \leq \hat{\sigma}(\hat{w}_{erc}) \leq \hat{\sigma}(w_{ew})$. 

Finally, figures \ref{rank_comp_40} and \ref{rank_comp_100} plot the ranks of the total cumulative portfolio Sharpe ratios (as per table \ref{test_perform}) against the mean of the 1-year non-overlapping ranks for all of the portfolios. The portfolios in the lower left-hand quadrant were ranked close to best overall regularly. The goal is to ensure the results of the SR ranking are not shifted by outlying results, which is done by checking whether the best overall performing methods are performing consistently. Therefore, the fact that we see a near 45 degree line (at least for N=40) is vindication of the out-of-sample consistent performance rankings of these portfolios. From the figures, it is clear that GMV portfolios were perennial outperformers relative to others.

\begin{figure}[H]
\centering
\begin{center}
\includegraphics[width=7.5cm]{n40_rank_com.pdf}
\end{center}
\centering
\caption{Total cumulative rank against the mean 1-year non-overlapping ranks for all of the 40-stock portfolios.}
\label{rank_comp_40}
\end{figure}
\begin{figure}[H]
\centering
\begin{center}
\includegraphics[width=7.5cm]{n100_rank_com.pdf}
\end{center}
\centering
\caption{Final rank against the mean 1-yearr non-overlapping ranks for all of the 100-stock portfolios.}
\label{rank_comp_100}
\end{figure}


\chapter{Conclusion}

This dissertation introduces a flexible framework for risk-based investing that is a general tool and allows the user to incorporate different estimation techniques into their risk-based portfolio construction. The framework and risk-based investing are empirically sound when tested with South African data, as all risk-based portfolios outperformed the market weight portfolio using the Sharpe ratio measure. The superiority of risk-based portfolios is consistent with similar results for US equities found by \cite{DGL07} and \cite{K10}.

Within risk-based portfolios, we found that GMV portfolios performed the best using the same Sharpe ratio measure. Furthermore, broadening the asset universe to include more assets materially improved portfolio performance.

No techniques outperformed the standard sample covariance matrix technique for finding risk-based portfolios across all risk-based portfolios, which is consistent with the findings of \cite{K10}. However, they propose longer estimation windows (20-years or 10-years) than were used in this study. Due to the similar performance of shrinkage techniques and standard sample estimation techniques, the period of this study (7.69-years) is close to an empirical break-even performance point between sample estimation and lack-of-data adjusted estimation strategies. 

Quantile factor modelling with regime switching worked well for the smaller and more concentrated GMV portfolios, where estimating a small number of substantial allocations correctly is essential. Subset resampling performed well for the less concentrated ERC portfolios, where determining the greatest number of allocations is necessary. Ridge regression shrinkage techniques have historically performed well in other studies. Still, the long-only constraint and a restrictive maximum allocation constraint of 10\% have the effect of regularising the optimisation in a manner already shown by \cite{JM03}. Therefore, the impact of ridge regression on performance is diminished.

Avenues for further research include using the flexible investing framework with other risk-based portfolios and penalising the objective function with alternate functions. The structure may also be modified to include different constraints. Further experiments may also be performed with varying universes of assets. Additionally, the general portfolio estimation procedure could be performed in risk factor space instead of the asset space.


% ----------------------------------------------------------------

% The bibliography is compiled from the file Refs, included in the template.
\addcontentsline{toc}{chapter}{Bibliography}
% The bibliography style file dcu_etal is also included in the template.
\bibliographystyle{dcu_etal} 

\bibliography{Refs}

\newpage
\appendix


\chapter{Abbreviations}

\begin{table} [h]
\caption{Portfolio types abbreviations.}
\centering
 \begin{tabular}{l  r} 
\hline
\textbf{Full Name} &  \textbf{Abbreviation}\\ 
 \hline
 Mean-variance optimal & MVO \\
 Global minimum variance & GMV \\
 Equal weight & EW \\
 Equal risk contribution & ERC \\
 Market weight & MW \\
 Maximum Sharpe ratio & MSR \\
 \hline
\end{tabular}
\end{table}

\begin{table} [h]
\caption{Metric abbreviations.}
\centering
 \begin{tabular}{l r} 
 \hline
\textbf{Full Name} &  \textbf{Abbreviation}\\ 
 \hline
 Marginal risk contribution & MRC \\
 Total risk contribution & TRC \\
 Inverse Herfindahl index & IHI \\
 Risk-weights & RW \\
 Marginal risk-weights & MRW \\
 Total risk-weights & TRW \\
 Sharpe ratio & SR \\
 Squared Mahalanobis distance & SMD \\
 Mean square error & MSE \\
 Turnover rate & TO \\
 Maximum drawdown & MDD \\
 \hline
\end{tabular}

\end{table}

\begin{table} [h]
\caption{Estimation techniques.}
\centering
 \begin{tabular}{l r} 
 \hline
\textbf{Full Name} &  \textbf{Abbreviation}\\ 
 \hline
Sample covariance matrix & SCM \\
Regime switching & RS \\
Quantile factor modelling & QFM \\
Quantile factor modelling with regime switching & QRS \\
Ridge regression & RR \\
Subset re-sampling & SRS \\
 \hline
\end{tabular}
\end{table}

\chapter{Portfolio mathematics}

\section{Marginal risk contribution} \label{MRC_app}

The portfolio beta interpretation of MRC:
\begin{align*}
\frac{\partial}{\partial w_i} \Big [ \sigma(w) \Big ] & = \frac{\partial}{\partial w_i}  \Big [ (w^\intercal \Sigma w)^{\frac{1}{2}} \Big ], &  \\
& = \frac{1}{2} \cdot (w^\intercal \Sigma w)^{-\frac{1}{2}} \cdot \frac{\partial}{\partial w_i} \Big [ w^\intercal \Sigma w \Big ], &\footnotesize \text{(chain rule)} \\
& = \frac{\frac{1}{2}\cdot 2 \cdot (\Sigma w)_i}{\sqrt{w^\intercal \Sigma w}}, & \footnotesize \text{(as given in chapter \ref{rb_ports})} \\
& = \frac{\mathbb{COV}[R_i, R_p]}{\sqrt{w^\intercal \Sigma w}}, & \footnotesize  \text{($R_i$- returns on asset i)} \\
& = \mathbb{SD}[(R_p)] \cdot  \frac{\mathbb{COV}[R_i, R_p]}{\mathbb{VAR}[(R_p)]}, \\
& = \mathbb{SD}[(R_p)] \cdot \beta_{i, p}\;.
\end{align*}

Minimising the weighted sum of the of the MRC's is equivalently the lowest beta portfolio with the single risk factor of the portfolio itself, which is the GMV portfolio.
\begin{align*}
\sum_{i = 1}^N w_i \cdot \frac{\partial}{\partial w_i} \Big [ \sigma(w) \Big ] & = \frac{\sum_{i = 1}^N w_i \cdot (\Sigma w)_i}{\sqrt{w^\intercal \Sigma w}}, & \\
\underset{w}{\text{argmin}} \Big \{\sum_{i = 1}^N w_i \cdot \frac{\partial}{\partial w_i} \Big [ \sigma(w) \Big ]  \Big \} & = \underset{w}{\text{argmin}} \Big \{ \sqrt{w^\intercal \Sigma w} \Big\},&   \footnotesize\text{(Taking the minimum on   both sides)}\\
& = \underset{w}{\text{argmin}} \Big \{w^\intercal \Sigma w \} . & \footnotesize \text{($\sqrt{\cdot}$ monotonic)}\\
\end{align*}

\section{Minimum weight constrained IHI} \label{IHI_max}

The maximum possible weight allocation for a single asset is $\alpha$. This allocation should be given to $\lfloor \frac{1}{\alpha} \rfloor$ assets where $\lfloor \cdot \rfloor$ is the truncation function. An additional weight $j$ should be set to satisfy the budget constraint, hence it should be $w_j = 1 - \lfloor \frac{1}{\alpha} \rfloor \cdot \alpha$. All of the other weights should be set to 0. Then the lower bound is:
\begin{align*}
H_{\text{lb}}^{-1} & = \frac{1}{\lfloor \frac{1}{\alpha} \rfloor \cdot \alpha^2 + (1- \lfloor \frac{1}{\alpha}\rfloor \cdot \alpha)^2} \;\; .
\end{align*}
 

\section{Proof of the volatility order inequality} \label{vol_ineq}

Consider a GMV framework where the IHI is bounded from below by a constant $c$:
\begin{align*} 
w^\diamond = \underset{w}{\text{argmin}} \Big \{ w^\intercal \Sigma w \Big \},
\end{align*}
subject to the constraints:
\begin{align*}
\mathcal{C}(w) &= 
\begin{cases}
H^{-1}(w) \geq c \\
w^\intercal \underline{1} = 1 \;\; \\
0 \leq w_i \leq \alpha, \; \forall i \;\;\; .
\end{cases}
\end{align*}

If $c \leq H^{-1}_{lb}$ then the constraint has no effect and the GMV portfolio is recovered. If $c = N$ then the only feasible solution is the EW portfolio. If $c > N$ there are no feasible solutions to the problem. The portfolio volatility is an increasing function of $c$ therefore we can deduce that: $$\sigma(w_{gmv}) \leq \sigma(w^\diamond(c)) \leq \sigma (w_{ew}).$$

It remains to show that the ERC portfolio is a $w^\diamond$ portfolio. Consider replacing the IHI constraint above with a log-constraint, $\sum_i \ln(w_i) \geq c$. If $c = - \infty$ then the GMV portfolio is recovered. If $c = -n \ln(n)$ then the EW portfolio is recovered, and if $c > -n\ln(n)$ then there are no feasible solutions. The portfolio volatility is also an increasing monotonic function of c therefore the inequality is replicated for a scaled choice of $c$ and by extension the ERC portfolio.
\section{Maximum Sharpe ratio risk-based portfolios} \label{msr_rb}

As mentioned in the text \cite{S07} has shown that the marginal sharpe ratios are equalised for the MSR portfolio as below:
\begin{align} \label{optim_eq}
\frac{\mu_i}{\text{MRC}_i} & = \frac{\mu_j}{\text{MRC}_j} \;\;\; \forall i, j \in \{1, ..., N \},
\end{align}
where $\mu_k$ represents the marginal excess return of asset k. Separately \cite{J13} showed that the EW, GMV and ERC portfolios could be found by the equalisation strategy:
\begin{align} \label{port_eq}
w_i^\gamma \sigma^{-\delta}_i \text{MRC}_i & = w_j^\gamma \sigma^{-\delta}_j \text{MRC}_j \;\;\; \forall i, j \in \{1, ..., N \},
\end{align}
where the choice of $\gamma$ and $\delta$ defines the portfolio. Combining the portfolio condition from equality \ref{port_eq} with the optimality equality \ref{optim_eq} yields the optimality condition for a given portfolio:
\begin{align}
w_i^\gamma \sigma_i^{(1- \delta)} \text{SR}_i & = w_j^\gamma \sigma_j^{(1- \delta)} \text{SR}_j \;\;\; \forall i, j \in \{1, ..., N \},
\end{align}
where $\text{SR}_k = \frac{\mu_k}{\sigma_k}$. Hence, a risk-based portfolio is optimal when constituents have equal weighted risk-adjusted SRs.

The EW portfolio can be analysed by setting $(\gamma, \delta) = (\infty, 0)$. For equlities \ref{optim_eq} and \ref{port_eq} to be jointly true when $w_i = \frac{1}{N}$ then: $$\text{MRC}_i = \text{MRC}_j \;\;\; \forall i, j \in \{1, ..., N \},$$
and there needs to be identical excess returns between assets. This means there should also be identical volatilities and identical correlations between all assets (i.e. $\Sigma = \rho\sigma^2 \underline{1} \,\underline{1}^\intercal - \rho I$). The GMV portfolio can be analysed by setting $(\gamma, \delta) = (0,0)$, which yields the optimality condition: $$\mu_i = \mu_j \;\;\; \forall i, j \in \{1, ..., N \}.$$
Hence, identical excess returns are required only. The ERC portfolio can be analysed by setting $(\gamma, \delta) = (1, 0)$, which  means the optimality condition becomes:
$$w_i \sigma_i \text{SR}_i = w_j \sigma_j \text{SR}_j \;\;\; \forall i, j \in \{1, ..., N \}.$$
Assuming constant correlation, $\rho_{i, j} = \rho$, the ERC portfolio allocations are given by the weighted inverse volatilities in the portfolio $w_i = \frac{\sigma^{-1}_i}{\sum_{k = 1}^N \sigma^{-1}_k}$ \citep{MRT10}. The equality of SRs is then the requirement for the above shown condition to hold. Therefore, identical correlations and SRs  ensure that the ERC portfolio coincides with the MSR portfolio.

\chapter{Reducing estimation error} 


\section{Quantile factor modelling example} \label{qfm_e}

Consider the QFM (from equation \ref{qfm}) with two factors, the market risk factor $R_m$ and the squares of that factor as an additional factor $R_m^2$. The factor loadings can be estimated by a procedure outlined by \cite{KB78}. Once the quantile errors have been deduced, the quantile factor sensitivities have been estimated, and the error quantiles set to the median quantile as in the main text the following quantile prediction model can be used:
\begin{align}
\hat{\mathcal{Q}}(\tau) = \hat{\alpha} (\tau) + \hat{\beta}_1^\intercal(\tau)R_m + \hat{\beta}_2^\intercal(\tau)R_m^2 + \epsilon_t(0.5),
\end{align}
which yields $|\tau|$ sample covariance matrices $\hat{\Sigma}^{(\tau)}$.

\section{Estimating the ERC portfolio Lagrangian multiplier} \label{eta_est}

We want to choose a value of $\eta$ that minimises all of the distances between the total risk contributions so that they are minimised, but this should be done across all $K$ folds using cross-validation. The general optimisation is the same as before:
\begin{align}
\eta^* = \underset{\eta}{\text{argmin}} \Big \{ \frac{1}{K} \sum_{k = 1}^K \hat{h}_k^{erc}(\eta, \mathcal{I}_k, \mathcal{I}_{-k})\Big \}.
\end{align}
The procedure of estimating $h$ still needs to be specified. The function $f(\cdot|\textbf{X})$ used to find the ERC portfolio can be used as a distance to minimise. Hence:
\begin{align}
h_k^{erc}(\eta, \mathcal{I}_k, \mathcal{I}_{-k}) = \sum_{i = 1}^N  \sum_{j \geq i}^N(w_{i, \mathcal{I}_{-k}}(\Sigma_{\mathcal{I}_k} w_{\mathcal{I}_{-k}})_i - w_{j, \mathcal{I}_{-k}}(\Sigma_{\mathcal{I}_k} w_{\mathcal{I}_{-k}})_j )^2,
\end{align}
 where $w$ and $\Sigma$ can be replaced by their sample estimations to find $\hat{h}$ given in the main text.

\end{document}
