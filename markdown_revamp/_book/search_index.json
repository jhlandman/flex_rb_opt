[
["index.html", "1 About", " Flexible risk-based portfolio optimisation (Github link) Emlyn Flint1, Jayson Landman2 2020-07-12 Abstract The purpose of this study is to present and test a general framework for risk-based investing. It permits various risk-based portfolios such as the global minimum variance, equal risk contribution and equal weight portfolios. The framework also allows for different estimation techniques to be used in finding the portfolios. The design of the study is to collate the existing research on risk-based investing, to analyse some modern methods to reduce estimation risk, to incorporate them in a single coherent framework, and to test the result with South African equity data. The techniques to reduce estimation risk draw from the usual mean-variance and risk-based optimisation literature. The techniques include regime switching, quantile regression, regularisation and subset resampling. In the South African experiment, risk-based portfolios materially outperformed the market weight portfolio out-of-sample using a Sharpe ratio measure. Additionally, the global minimum variance portfolio performed better than other risk-based portfolios. Given the long estimation window, no estimation techniques consistently outperformed the application of sample estimators only. Keywords: risk-based investing, portfolio optimisaiton, estimation risk. 1 About This is a research paper created with the bookdown package in R. It is intended to promote reproducibility in academic research. Legae Peresec↩︎ Ninety One Asset Management↩︎ "],
["introduction.html", "2 Introduction", " 2 Introduction If a risk-averse investor wants to construct portfolios with desirable properties, they would ideally want to find allocations that offer an attractive risk-reward trade-off. Markowitz (1952) developed modern portfolio theory and introduced the mean-variance optimal portfolio as a quantitative solution to this asset allocation problem. However, the reward derived from this portfolio has to be estimated from sample data and is often difficult to accurately predict - which, in turn, leads to Markowitz’s mean-variance portfolio being highly sensitive to the estimated portfolio inputs. As an alternative, risk-based investing provides an avenue for finding portfolios for which expected returns do not need to be estimated, and therefore resolves the portfolio expected return sensitivity problem. Examples of risk-based portfolios commonly seen in practice are the global minimum variance portfolio, the equal risk contribution portfolio, and the equal weight portfolio. These three portfolios are optimal for investors that prioritise weight diversification, risk diversification, or a specific combination of both. Nevertheless, risk-based portfolios remain sensitive to covariance matrix estimation and hence estimation risk. Improving risk-based estimation is done in three ways in this research. The first improvement alters the covariance estimation procedure by accounting for differences in the sample data. These changes include grouping to account for both non-normality and state-based inhomogeneity. The second involves penalising the optimisation to limit the range of admissable portfolios, which increases the investor’s odds of choosing a well-estimated portfolio. The final enhancement changes the implementation methodology entirely by performing the portfolio optimisation on subsets of assets and then resampling to find an aggregate portfolio. This research aims to bring together useful elements of risk-based portfolio estimation and construction methodology into a single flexible framework. The general structure allows a choice of risk-based portfolio as well as estimation risk reduction technique to improve the out-of-sample portfolio performance. Once we have established a framework, the specific portfolio and estimation technique, examples are developed theoretically. All of these reforms will be hollow without being applied to actual financial data. Therefore, the various estimation techniques and risk-based portfolio pairs are back-tested using South African equity data in an experiment, with the results being measured by standard performance methodologies. This research is built on the work of several different authors. Firstly - as with nearly all portfolio construction research - this dissertation hinges on the modern portfolio theory of Markowitz (1952). It then considers the particular case of risk-based investing and makes use of the generalised frameworks introduced by Jurczenko, Michel, and Teiletche (2013) and Richard and Roncalli (2015). Finally, in terms of improving the estimation and optimisation processes, we make use of the ideas investigated by Flint and Du Plooy (2018), Kinn (2018), and Shen and Wang (2017). The rest of this dissertation is set out as follows. Chapter 2 outlines a general framework for constructing risk-based portfolios and estimating them in a robust manner. Chapter 3 gives an overview of the specific risk-based portfolios considered in this work. Chapter 4 presents several techniques for reducing estimation risk, exploring their theoretical underpinnings and providing general intuition. Chapter 5 then considers the empirical application of these techniques, highlighting several technicalities. Chapter 6 then applies the flexible risk-based framework to SA equity data, providing an empirical comparison of different implementations. Chapter 7 concludes the research and provides avenues for further research. References "],
["genframework.html", "3 A framework for constructing risk-based portfolios 3.1 An overview of modern portfolio theory 3.2 Introducing risk-based investing 3.3 Improving risk-based portfolios", " 3 A framework for constructing risk-based portfolios 3.1 An overview of modern portfolio theory Every investor has a universe of \\(N\\) assets to which they can allocate capital. The proportion of their allocation to the i\\(^{\\text{th}}\\) asset, \\(w_i\\), depends on the investor’s risk and return preferences. They could prefer riskier asset combinations because they require high capital growth, or they could prefer more stable asset combinations that prioritise capital preservation. The column vector \\(w = [w_1,w_2,\\text{…},w_N]^\\intercal\\) is an \\(N \\times 1\\) vector of allocations that define an investor’s portfolio. This portfolio is constrained by the investor’s capital budget, which may be articulated through the notion of weights. Therefore, all considered portfolios should adhere to the budget constraint: \\(\\sum_i w_i = 1\\). In his seminal paper, Markowitz (1952) attempts to quantify the asset allocation process. Herein he posits that the investor has to make a risk-reward trade-off when considering their portfolio returns, \\(R_\\mathrm{p}\\), over a pre-determined time horizon \\([0, T]\\). In his framework, he measures risk with the variance of portfolio returns \\(\\mathbb{V}\\mathrm{ar}[R_\\mathrm{p}] = w^\\intercal \\Sigma w\\), where \\(\\Sigma\\) is the \\(N \\times N\\) asset return covariance matrix. Markowitz measures reward by the expectation of portfolio returns \\(\\mathbb{E}[R_\\mathrm{p}] = w^\\intercal \\mu\\), where \\(\\mu\\) is the \\(N \\times 1\\) vector of expected asset returns. If the investor fixes expected returns to a constant, \\(\\mathbb{E}[R_\\mathrm{p}] = c\\), they encounter the problem of minimising portfolio risk, \\(\\mathbb{V}\\mathrm{ar}[R_\\mathrm{p}]\\). The mean-variance optimal (MVO) portfolio \\(w_\\mathrm{mvo}\\) achieves this goal while adhering to the budget constraint and the fixed expected return constraint. Written mathematically, \\(w_\\mathrm{mvo}\\) is the solution to the following Markowitz optimisation: \\[\\begin{align} w_\\mathrm{mvo} &amp;= \\underset{w}{\\text{argmin}} \\Big \\{ w^\\intercal \\Sigma w \\Big \\}, \\tag{3.1} \\end{align}\\] subject to the constraints: \\[\\begin{align*} \\mathcal{C}(w) &amp;= \\begin{cases} w^\\intercal \\mu &amp;= c \\;\\;\\; \\\\ w^\\intercal \\underline{1} &amp;= 1 \\;\\; . \\end{cases} \\end{align*}\\] There is a complication when using this framework in practice because \\(\\Sigma\\) and \\(\\mu\\) are unknown. The investor can only observe a small set of sample returns from the underlying stock processes that use these quantities as inputs. The sample returns, represented by a \\(N \\times T\\) matrix \\(\\textbf{X}\\), can be used to estimate \\(\\Sigma\\) and \\(\\mu\\). The sample estimates are: \\[\\begin{align} \\textbf{S}&amp;= \\frac{1}{T - 1}(\\textbf{X} - \\mathbf{\\bar{x}} \\underline{1}^\\intercal_T)(\\textbf{X} - \\mathbf{\\bar{x}} \\underline{1}^\\intercal_T)^\\intercal, \\tag{3.2} \\\\ \\mathbf{\\hat{\\mu}} &amp;= \\mathbf{\\bar{x}} , \\tag{3.3} \\end{align}\\] where \\(\\mathbf{\\bar{x}}\\) is a \\(N \\times 1\\) vector of mean returns. A total of \\(\\frac{N(N+1)}{2}\\) distinct parameters are being estimated for the sample covariance matrix (SCM) while \\(N\\) distinct sample expected returns are estimated. \\(\\textbf{S}\\) and \\(\\hat{\\mu}\\) can be substituted into equation (3.1) to infer an MVO portfolio, \\(\\hat{w}_\\mathrm{mvo}\\). For each level of portfolio return \\(c = \\hat{w}^\\intercal \\hat{\\mu}\\), an estimated portfolio volatility level \\(\\mathbb{SD}[\\hat{R}_\\mathrm{p}] = \\sqrt{\\hat{w}^\\intercal\\textbf{S}\\hat{w}}\\) is realised. These estimated expected return-volatility pairs induce a frontier in the expected return-volatility plane, which may or may not be close to the frontier implied by the actual inputs \\(\\Sigma\\) and \\(\\mu\\). The actual frontier is optimal for the Markowitz framework, and he terms it the `efficient frontier’. The above framework and estimation procedure yields a solution to the asset allocation dilemma, but there is still a sensitivity predicament. Michaud (1989) shows that the MVO procedure, as described above, overweights assets with substantial estimated returns \\(\\mathbf{\\bar{x}}\\). However, these are the same assets that are likely to have been misestimated. Thus, any potential estimation errors are `maximised’ - an undesirable property that makes \\(\\hat{w}_\\mathrm{mvo}\\) a potential liability for the investor to hold. The MVO procedure is commonly adjusted to reduce sensitivity to the input \\(\\mathbf{\\bar{x}}\\) in one of two ways. The first is to estimate expected returns in a manner that targets return drivers or factors, which has motivated the rise of factor-based investing (Ang 2014). The second adjustment is to remove the dependency on expected return estimates altogether and only construct portfolios based on their risk properties. The latter has inspired the field of risk-based investing and is the focus of this dissertation. 3.2 Introducing risk-based investing A general risk-based portfolio optimisation programme is given below in equation (3.4). Removing the expected return constraint and altering the previous MVO optimisation problem for the consideration of a generalised objective risk function \\(f(\\cdot| \\textbf{X})\\) yields: \\[\\begin{align} w^* = \\underset{w}{\\text{argmin}} \\Big \\{ f(w, \\Sigma| \\textbf{X}) \\Big \\}, \\tag{3.4} \\end{align}\\] subject to the constraint: \\[\\begin{align*} w^\\intercal \\underline{1} &amp;= 1 \\;\\; , \\end{align*}\\] where \\(f(\\cdot | \\textbf{X})\\) is a risk metric to be minimised. The choice of \\(f(\\cdot | \\textbf{X})\\) determines which risk-based portfolio is the optimal solution. Chapter make chapeter expounds on the risk-based portfolio types relevant to this research. The range of feasible portfolios given by equation (3.4) is practically too general because unlikely single asset weights are still possible. To this end, an investor should apply a weight constraint to limit feasible allocations, ensuring comparability with practical investing. Jagannathan and Ma (2003) show that risk-based long-short portfolios can have extreme weights in practice, which are unlikely to be accepted by an investor. Additionally, Jagannathan and Ma (2003) also conclude that imposing the long-only investment constraint on US equities leads to improved efficiency for optimal portfolios constructed with the first two sample moments \\(\\hat{\\mu}\\) and \\(\\textbf{S}\\). Hence, applying the long-only constraint is both statistically appropriate and practically relevant for most investors. Equally important is that an investor will be reluctant to concentrate their portfolio in a small number of assets. Limiting the maximum single asset allocation to a selected weight \\(\\alpha\\) avoids such concentration. These constraints are concurrently expressed as \\(\\{w :0 \\leq w_i \\leq \\alpha, \\; \\forall i \\}\\) and can be added to optimisation (3.4). In its current form, the developed framework is still somewhat abstract, so it is not obvious how to improve it. Even so, one can always define specific properties that the framework ought to have for there to be a good chance of it operating as intended. 3.3 Improving risk-based portfolios Hadamard (1923) defines a mathematical problem as ‘well-posed’ if: the solution exists, the solution is unique, the solution is not overly sensitive to small perturbations in inputs. Well-posed problems are easier to work with and are more stable than ill-posed problems - ones that fall short of the definition. The expected return constraint was previously disregarded for MVO portfolios because the MVO framework often does not meet requirement 3. when the sample mean returns estimate \\(\\mu\\). Risk-based portfolios are therefore more `well-posed’ than MVO portfolios. However, there are two common ways in which risk-based portfolios are also ill-posed. The first is if there are very few sample observations; namely, if \\(T &lt; N\\). In this scenario, the covariance matrix is not of full rank and is therefore not invertible, causing non-unique solutions to \\(w^*\\). The framework is then ill-posed by 2. The second is if small changes in \\(\\textbf{X}\\) result in large deviations of \\(w^*\\). The framework is then ill-posed by 3. Many researchers such as Jobson and Korkie (1981) and Best and Grauer (1991) have shown that the latter phenomenon is observed in practice, and often persists even if \\(T\\) is much larger than \\(N\\). To address point (iii) and make the problem well-posed, one needs a measure of sensitivity. We outline a pseudo-derivation of a sensitivity measure below. Kinn (2018) views the portfolio optimisation problem from a modelling perspective. The returns on the portfolio are modelled directly by an unknown function \\(g(\\cdot)\\): \\[\\begin{align} R_p &amp; = g(\\textbf{X}) + \\epsilon, \\tag{3.5} \\end{align}\\] where \\(\\epsilon\\) has the normal distribution with zero mean and variance \\(\\phi^2\\). Estimating \\(g(\\cdot)\\) is the aim of using the framework, and it is a real-valued function that is not necessarily differentiable or continuous. Each algorithm \\(q\\) refers to a combination of estimation procedure for the inputs, and a computation of a risk-based portfolio using equation (3.5). Each \\(q\\) specifies an estimate of the function \\(g\\), denoted \\(\\hat{g}_q\\). In the same way that \\(\\hat{w}^*\\) (\\(w^*\\) calculated with sample inputs \\(\\textbf{S}\\) and \\(\\hat{\\mu}\\)) can be used to estimate the out-of-sample risk-based portfolio, \\(\\hat{g}_q\\) can predict out-of-sample portfolio returns, which are forecasted most accurately by the unobservable function \\(g\\). It is necessary to distinguish between the two types of data that are available. The first is historical data comprising of the matrix \\(\\textbf{X}_0\\) and in-sample returns \\(R_{\\mathrm{p}, 0}\\), which combine to form the set \\(\\mathcal{H}_0\\). The second is out-of-sample data \\(\\textbf{X}_1\\) and \\(R_\\mathrm{p, 1}\\), which combine to form the set \\(\\mathcal{H}_1\\). Algorithm \\(q\\) does not utilise the data contained in \\(\\mathcal{H}_1\\), which are chronologically realised after the most recent points in \\(\\mathcal{H}_0\\). A mean squared error penalty is appropriate to measure the accuracy with which \\(\\hat{g}_q (\\textbf{X}_1)\\) (estimated using \\(\\mathcal{H}_0\\)) predicts the out of sample returns \\(R_\\mathrm{p, 1}\\). Kinn (2018) terms the expectation of the out-of-sample mean squared error “generalisation error” (GE), shown mathematically as: \\[\\begin{align} GE(\\hat{g}_q) &amp; = \\mathbb{E}[(R_\\mathrm{p, 1} - \\hat{g}_q (\\textbf{X}_1))^2|{\\mathcal{H}_1}], \\end{align}\\] where GE is specific to a sample. The actual quantity of interest is the expected performance of \\(q\\) for many potential sample sets, as we are evaluating \\(q\\)’s efficacy holistically. This quantity is called the expected generalisation error across all samples, denoted \\(G_q\\). Furthermore, \\(G_q\\) may be decomposed3 to reflect a common modelling trade-off between bias and variance: \\[\\begin{align} G_q &amp;:= \\mathbb{E} \\Big [ \\mathbb{E}[(R_\\mathrm{p, 1} - \\hat{g}_q (\\textbf{X}_1))^2|{\\mathcal{H}_1}] \\Big | \\mathcal{H}_0\\Big ] , \\\\ &amp; = \\underbrace{ \\Big (g(\\textbf{X}_1) - \\mathbb{E}[\\hat{g}_q (\\textbf{X}_1)|\\mathcal{H}_0] \\Big )^2}_{\\text{squared bias}} + \\underbrace{\\mathbb{V}\\mathrm{ar} [\\hat{g}_q(\\textbf{X}_1)|\\mathcal{H}_0]}_{\\text{variance}} + \\underbrace{\\phi^2}_{\\text{irreducible error}}.\\tag{3.6} \\end{align}\\] The squared bias is the extent to which the expectation of predicted returns differs from the best possible predictor of returns, the correct function \\(g(\\cdot)\\). The variance measures the magnitude by which the predicted returns will vary under repeated sampling. By setting \\(\\hat{g}_q(\\cdot) = g(\\cdot)\\) in equation (3.6) only statistical noise remains; hence, the noise is irreducible. The risk of misestimating \\(g(\\cdot)\\) should not include the risk that is retained by even the best estimator. Therefore, estimation risk is considered as the sum of the first two terms only, the squared bias and the variance. An over-fitted algorithm will have high variation for repeated samples. An under-fitted algorithm will have high bias and be consistently poor for repeated samples. The over- and under-fitting trade-off is an example of how the bias-variance trade-off works in practice. Until now, we have assumed that the estimated portfolio \\(\\hat{w}^*\\) from equation (3.4) is an unbiased estimate of the actual risk metric-minimising portfolio \\(w^*\\) because the choice of \\(f(\\cdot | \\textbf{X})\\) determines precisely the type of risk-based portfolio. However, \\(f(\\cdot|\\textbf{X})\\) does not precisely determine the estimation risk. Employing a penalty on the objective function introduces bias to reduce estimation risk, i.e. hopefully, the squared bias increase does not outweigh the variance decrease. The introduction of the penalty yields an estimated portfolio \\(\\hat{w}^*\\) that is consistently closer to \\(w^*\\) than an unbiased portfolio would be. The penalty constraint can be stated as \\(P(w) \\leq s\\) and reduces the set of all possible portfolios. If done correctly allocations that are misestimated by the heftiest margins are excluded by this constraint, and allocations that are consistently closer to the actual portfolio remain. The general risk-based framework can now be restated as below to accommodate the penalty using a Lagrangian multiplier approach: \\[\\begin{align} w^* = \\underset{w}{\\text{argmin}} \\Big \\{ f(w, \\Sigma| \\textbf{X}) + \\lambda P(w) \\Big \\}, \\tag{3.7} \\end{align}\\] subject to the constraints: \\[\\begin{align*} \\mathcal{C}(w) &amp;= \\begin{cases} w^\\intercal \\underline{1} = 1 \\;\\; \\\\ 0 \\leq w_i \\leq \\alpha, \\; \\forall i \\;\\;\\; , \\end{cases} \\end{align*}\\] where \\(P(\\cdot)\\) is the penalty function, and \\(\\lambda\\) is the Lagrangian multiplier. Kinn proposes estimating \\(\\lambda\\) in a way that is consistent with the rest of the optimisation. Therefore, we apply the \\(\\lambda\\) estimation that minimises the portfolio specific risk metric using in-sample data. Additionally, by setting \\(\\lambda = 0\\), the unpenalised portfolio can still be recovered. Optimisation (3.7) will be the general risk-based portfolio optimisation going forward. Improving on the vanilla risk-based optimisation is done in three ways in this research, and these improvements are also common in the literature. Approach one deals with the assumption in equation (3.5) that \\(\\epsilon\\) has a normal distribution, and that the errors through time are independent and identically distributed. If \\(\\hat{g}_q\\) fails to approximate \\(g\\) correctly, then the assumption is violated. There could be natural heterogeneity in the data accounted for by \\(g\\). Flint and Du Plooy (2018) attempt to deal with heterogeneity by changing the estimation procedure of \\({g}\\) so that it includes and accounts for potential differences in each observation of sample data. They do this through the application of regimes and quantiles, grouping the input data to increase the accuracy of the estimated model. The second approach adapted from Kinn (2018) has already been shown above and involves penalising the optimisation and setting \\(\\lambda &gt; 0\\) in equation (3.7). The third and final approach requires resampling from the observations in \\(\\textbf{X}\\) for different subsets of assets and hinges on implementation adjustments. Shen and Wang (2017) suggest finding \\(w^*\\) multiple times for different resampled subsets and blending the result afterwards to find an aggregate weight, that hopefully reduces estimation risk. Chapter insert chapter ref contains a more detailed exploration of these techniques. Before that, further investigation of desirable risk properties and the portfolios that have them is required to specify \\(f(\\cdot|\\textbf{X})\\). References "],
["rbportch.html", "4 Overview of risk-based portfolios 4.1 Risk-based portfolio types 4.2 Risk-based portfolio properties", " 4 Overview of risk-based portfolios There are many types of risk-based portfolios, for a broader analysis refer to Plessis and Rensburg (2017). In this chapter, we review three risk-based portfolios: the global minimum variance, equal weight and equal risk contribution portfolios. The risk properties that these portfolios have, the strategies that bear them out and the conditions under which they perform optimally are all covered. 4.1 Risk-based portfolio types Each risk-based portfolio is optimal in some sense because they all minimise a specific objective risk function. The nature of the objective function depends on the investor’s risk preferences. In line with Markowitz (1952), a genetic risk metric to minimise is the portfolio variance, as an investor may not be willing to tolerate large swings in capital value. Setting \\(f(\\cdot|\\textbf{X}) = w^\\intercal \\Sigma w\\) in portfolio optimisation (3.7) yields the global minimum variance (GMV) portfolio, denoted \\(w_\\textrm{gmv}\\). The objective function is the same as for the MVO portfolio for an imputed value of \\(c\\), meaning that the GMV portfolio sits on the efficient frontier. The GMV portfolio is also the only risk-based portfolio that is always on the efficient frontier. Maillard, Roncalli, and Teı̈letche (2010) utilise the concept of an asset’s marginal risk contribution (MRC) in a portfolio to perform risk-based portfolio calculations. It is the sensitivity of the portfolio volatility to the weight of an asset in a portfolio. Alternative representations of the MRC and an outline of why the weighted sum of the MRC’s is minimised for the GMV portfolio are shown in appendix insert appendix ref. Below is the mathematical definition of the MRC for the \\(i^{\\mathrm{th}}\\) asset: \\[\\begin{align} \\frac{\\partial}{\\partial w_i} \\Big [ \\sigma(w) \\Big ]= \\frac{(\\Sigma w)_i}{\\sqrt{w^\\intercal \\Sigma w}}. \\end{align}\\] When taking the realised portfolio returns as the single risk factor, there is no idiosyncratic risk present for assets in the portfolio against this factor. Accordingly, the GMV portfolio is the lowest possible beta portfolio. The GMV portfolio is, therefore, optimal for the investor that always takes on the least risk at the margin. However, risk has to be estimated, so the investor only knows what the least risky options available to them are in a historical sense. The downside is that always choosing the lowest marginal risk contributing asset is implicitly displaying a high level of confidence in estimations of risk. Contrastingly, the investor may be at a loss when estimating risk. The only measure of diversification for such an investor is weight diversification. To minimise their risk taken at the margin, this investor would hold the smallest possible weight in each of the assets while still satisfying the constraints. This strategy ensures that the investor avoids the maximum marginal risk contributing asset to the greatest extent possible given that they are unsure which of the \\(N\\) assets it is. The least weight concentrated portfolio that this strategy alludes to is the equal weight (EW) portfolio: \\[\\begin{align} w_{ew} &amp; = \\Big [\\frac{1}{N} \\; \\cdot \\cdot \\cdot \\frac{1}{N} \\Big ]^\\intercal \\; , \\end{align}\\] where the weight diversification measure that this portfolio maximises is the inverse Herfindahl index (IHI), calculated as \\(H^{-1} (w) = (\\sum_{i = 1}^N w_i^2)^{-1}\\). The most weight concentrated portfolio is the single asset portfolio - where a single non-zero asset weight is \\(1\\). The IHI of this portfolio is \\(1\\), which is as low as it can be. On the other hand, the EW portfolio has an IHI value of \\(N\\). All portfolios, therefore, have an IHI on the interval \\([1, N]\\). In the presence of a maximum weight constraint, the lower bound of the interval changes. The new lower bound is derived in appendix insert appendix ref. To find the EW portfolio using framework (3.7), set the objective function to the Herfindahl index, \\(f(\\cdot|\\textbf{X}) = \\sum_{i = 1}^N w_i^2\\). No parameters in the objective function require estimation, so this is a sample return independent optimisation, which reflects the investor’s lack of confidence in historical data. The GMV and EW portfolios represent two extremes of investors, specifically those that value volatility reduction only and those that value weight diversification only. The inequality \\(\\sigma(w_{gmv}) \\leq \\sigma(w^\\diamond) \\leq \\sigma(w_{ew})\\) verifies this intuition. The portfolio \\(w^\\diamond\\) has intermediate weight concentration. Proof of the inequality is given in appendix insert appendix ref. It is unlikely that a capital allocator will completely disregard one risk-based approach for another. One method to construct a sound intermediate portfolio that incorporates the MRC philosophy from the GMV portfolio construction and the high risk-asset avoidance philosophy from the EW portfolio construction is to equalise the total risk contribution (TRC) from each asset. An asset’s TRC is the product of its MRC and its weight in the portfolio: \\[\\begin{align*} \\text{TRC}_i &amp;= w_i \\cdot \\frac{\\partial}{\\partial w_i} \\Big [ \\sigma(w)\\Big ], \\\\ &amp; = \\frac{w_i \\cdot (\\Sigma w)_i}{\\sqrt{w^\\intercal \\Sigma w}}, \\end{align*}\\] where TRC’s sum to the portfolio volatility. An equal risk contribution (ERC) portfolio equalises all of the TRC’s so that no single asset is a comparatively significant contributor to risk. The choice of \\(f(\\cdot|\\textbf{X})\\) that minimises the squared distances between the TRC’s to the greatest extent possible is given below: \\[\\begin{align} f(w, \\Sigma|\\textbf{X}) &amp;= \\sum_{i = 1}^N \\sum_{j \\geq i}^N(w_i(\\Sigma w)_i - w_j(\\Sigma w)_j)^2 \\;\\; . \\end{align}\\] Maillard, Roncalli, and Teı̈letche (2010) show that a log-constraint on the weights in GMV optimisation could equivalently express this choice of \\(f(\\cdot| \\textbf{X})\\) - an idea explored further in appendix insert appendix ref. Therein it is shown that the ERC portfolio is an intermediate portfolio \\(w^\\diamond\\). While there are several other risk-based portfolios that have been suggested in the literature, we will focus our attention only on these three portfolios, which are arguably some of the most common risk-based portfolios seen in practice Jurczenko, Michel, and Teiletche (2013). 4.2 Risk-based portfolio properties Section 3.1 introduces the Markowitz efficient frontier. Sharpe (1964) extends this work to deduce that there is an optimal portfolio called the tangency portfolio. He does make certain assumptions about investors’ preferences and the presence of a risk-free asset. The market-weighted (MW) portfolio is the portfolio for which the efficient frontier is tangential to the line bisecting the y-axis at the risk-free rate (\\(r_\\mathrm{f}\\)) in the expected return-volatility plane. The MW portfolio is the portfolio held by all investors in the market on average and is relevant because it offers the investor diversification with negligible transaction costs Perold (2007). The MW portfolio is not risk-based in the traditional sense, but it does not require an estimate of expected returns to calculate; hence, the MW portfolio offers a cheap benchmark against which to compare risk-based portfolio performance. However, the holder of the MW does implicitly adopt all investors’ weighted expectations of expected returns Haugen and Baker (1991). The tangency portfolio is optimal for the Sharpe ratio (SR) measure under Sharpe’s assumptions. The measure is defined as: \\[\\begin{align} \\text{SR}_\\mathrm{p} = \\frac{\\mathbb{E}[R_\\mathrm{p}] - r_\\mathrm{f}}{\\mathbb{SD}[R_\\mathrm{p}]} \\;. \\end{align}\\] Within the MVO construction, the MW portfolio has the maximum Sharpe ratio (MSR) and is, therefore, the MSR portfolio. Scherer (2007) shows that the MSR portfolio, \\(w_\\mathrm{msr}\\), can alternatively be expressed as the portfolio for which marginal excess returns and the MRC’s are equal for all portfolio constituents. Jurczenko, Michel, and Teiletche (2013) use this fact to find MSR optimality conditions for each of the risk-based portfolios, some examples of which are shown in appendix insert appendix num. Table 4.1 summarises the salient risk properties of the EW, ERC, and GMV portfolios. Included is the strategy to find the portfolio, the requirements for when the portfolio coincides with the MSR portfolio, and their empirical risk characteristics. The risk characteristics entail whether the risk is inherent to the investment, the construction of the portfolio, or liquidity restrictions when creating the portfolio. Table 4.1: Risk-based investing portfolio properties Jurczenko, Michel, and Teiletche (2013) Portfolio | S Strategy | MS MSR conditions | R Risk characteristics | EW | E Equalise \\(w_i\\) | Id Identical excess returns. Identical volatilities. Identical correlations. | M Medium to high risk. Insensitive to \\(\\Sigma\\). Low turnover. | ERC | E Equalise \\(TRC_i\\) | Id Identical Sharpe ratios. Identical correlations. | M Medium risk. Moderately sensitive to \\(\\Sigma\\). Medium turnover. | GMV | E Equalise \\(MRC_i\\) | Id Identical excess returns. | L Lowest risk. Highly sensitive to \\(\\Sigma\\). High turnover. | While the MSR conditions are theoretically compelling, out-of-sample optimality is harder to determine in practice. Haugen and Baker (1991) show that portfolios that are superior to the MW portfolio exist when: short-selling is restricted, investments are taxed, and foreign investors are active market participants. These portfolios should have the same expected return as the MW portfolio with lower volatility. Their statement is true even in an `efficient market’. Studies of the historical performance show that some portfolios outperform others. In these studies, the authors restrict the asset universe to US equities; hence, their results will not necessarily translate to South Africa. The hope of introducing risk-based portfolios is to find Haugen and Baker’s superior portfolios. Evidence supporting this ambition exists. DeMiguel, Garlappi, and Uppal (2007) demonstrate the robust out-of-sample performance of EW portfolios when compared to MVO and MW portfolios for a broad range of asset universes. Clarke, De Silva, and Thorley (2006) also demonstrate that GMV portfolios show outperformance against the MW and MVO benchmarks. They initially attribute this to the diachronic persistence of covariances when compared to expected returns. In a later paper, Clarke, De Silva, and Thorley (2011) suggest that the outperformance is due to a bias inherent in the portfolio construction towards stocks that do not move with the rest of the market, but that still have comparatively high expected returns. Within risk-based portfolios, Kritzman, Page, and Turkington (2010) have shown that GMV portfolios outperform EW portfolios when the implementer uses a long enough estimation window. Therefore, they establish a defence for using optimisation on a sample covariance matrix. This research remains consistent with these findings, using the EW portfolio as a benchmark in pursuit of better out-of-sample performance within the GMV and ERC frameworks. In the next chapter, we outline the techniques used to achieve this aim. 4.2.1 Code to find portfolios library(rlang) library(magrittr) library(dplyr) library(data.table) library(tidyr) full_ret &lt;- readRDS(&quot;data/full_ret.rds&quot;) model_rebal_dat &lt;- readRDS(&quot;data/model_rebal_dat.rds&quot;) # MW returns and weights --------------------------------------------------------------------------- mw_month_rebal &lt;- full_ret %&gt;% group_by(.data$month_index) %&gt;% filter(.data$date == min(date)) %&gt;% ungroup() %&gt;% mutate(top40_ret_contrib = .data$month_ret * .data$top40_weight) %&gt;% mutate(top100_ret_contrib = .data$month_ret * .data$top100_weight) %&gt;% group_by(.data$month_index) %&gt;% mutate(top40_total_ret = sum(.data$top40_ret_contrib)) %&gt;% mutate(top100_total_ret = sum(.data$top100_ret_contrib)) %&gt;% ungroup() %&gt;% select( .data$month_index, .data$name, .data$month_ret, .data$top40_weight, .data$top40_ret_contrib, .data$top40_total_ret, .data$top100_weight, .data$top100_ret_contrib, .data$top100_total_ret ) # EW returns and weights --------------------------------------------------------------------------- ew_month_rebal &lt;- full_ret %&gt;% group_by(.data$month_index) %&gt;% filter(.data$date == min(date)) %&gt;% ungroup() %&gt;% mutate(ew40_weight = if_else(.data$top40_weight &gt; 0, 1 / 40, 0)) %&gt;% mutate(ew100_weight = if_else(.data$top100_weight &gt; 0, 1 / 100, 0)) %&gt;% mutate(ew40_ret_contrib = .data$month_ret * .data$ew40_weight) %&gt;% mutate(ew100_ret_contrib = .data$month_ret * .data$ew100_weight) %&gt;% group_by(.data$month_index) %&gt;% mutate(ew40_total_ret = sum(.data$ew40_ret_contrib)) %&gt;% mutate(ew100_total_ret = sum(.data$ew100_ret_contrib)) %&gt;% ungroup() %&gt;% select( .data$month_index, .data$name, .data$month_ret, .data$ew40_weight, .data$ew40_ret_contrib, .data$ew40_total_ret, .data$ew100_weight, .data$ew100_ret_contrib, .data$ew100_total_ret ) # GMV sample covar returns and weights ------------------------------------------------------------- out &lt;- model_rebal_dat$data[[1]] tmp &lt;- out %&gt;% select(.data$date, .data$name, .data$week_ret) %&gt;% pivot_wider(id_cols = c(&quot;date&quot;, &quot;name&quot;), names_from = c(&quot;name&quot;), values_from = c(&quot;week_ret&quot;)) %&gt;% select(-.data$date) # need to decide what to do with all of the NAs... should I fill them in with sampled data # could fill in with simulated data from industry, #%&gt;% # Writing files ------------------------------------------------------------------------------------ saveRDS(mw_month_rebal, file = &quot;data/port_out/mw_month_rebal.rds&quot;) saveRDS(ew_month_rebal, file = &quot;data/port_out/ew_month_rebal.rds&quot;) head(mw_month_rebal, 10) ## # A tibble: 10 x 9 ## month_index name month_ret top40_weight top40_ret_contrib top40_total_ret top100_weight top100_ret_contrib top100_total_ret ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21996 ABI 0.0133 0 0 -0.0315 0.00104 0.0000139 -0.0292 ## 2 21996 AXL 0.0436 0 0 -0.0315 0 0 -0.0292 ## 3 21996 ACL 0.0218 0 0 -0.0315 0 0 -0.0292 ## 4 21996 ACT 0.0833 0 0 -0.0315 0 0 -0.0292 ## 5 21996 ADR 0.250 0 0 -0.0315 0.0000499 0.0000125 -0.0292 ## 6 21996 AEL -0.265 0 0 -0.0315 0 0 -0.0292 ## 7 21996 AEN -0.130 0 0 -0.0315 0 0 -0.0292 ## 8 21996 AFE -0.106 0.00626 -0.000662 -0.0315 0.00598 -0.000633 -0.0292 ## 9 21996 AFI 0.105 0 0 -0.0315 0.00109 0.000115 -0.0292 ## 10 21996 AFL 0 0 0 -0.0315 0 0 -0.0292 References "],
["estriskreduce.html", "5 Estimation risk reduction techniques 5.1 Improving optimisation inputs 5.2 Penalising the optimization 5.3 Alternate implementation methods", " 5 Estimation risk reduction techniques As stated in chapter 3, estimation risk is comprised of squared bias and variance. There are many methods to approach reducing estimation risk, but in this research, we introduce three ways that are consistent with the general risk-based investing framework presented in equation (3.7). The first method deals with improving the estimation of the inputs to the risk-based portfolio function \\(f(w, \\Sigma|\\textbf{X})\\), accounting for heterogeneity in the input data. The second method involves penalising the optimisation objective function to obtain a portfolio estimate with consistently lower deviation from the actual out-of-sample risk-based portfolio solution. The final process entails changing the implementation method in a manner that reduces estimation risk. Every risk reduction technique falls into one of these three categories. 5.1 Improving optimisation inputs The first approach to improving on the sample ERC and GMV portfolios involves finding better estimates for the input \\(\\Sigma\\), given the set of sample returns. As stated in equation (3.5), the function \\(g\\) deals with the sample returns in a manner that ensures the irreducible error, \\(\\phi^2\\), is independent and identically normally distributed. However, because \\(g\\) is unobservable, our estimation \\(\\hat{g}\\) might not ensure this property. Heterogeneity of sample errors for investment portfolios has been observed in empirical finance by Ang and Chen (2002), who demonstrate that negative stock price correlations are less pronounced in downward markets. Therefore, empirical finance suggests two states of the world, one where the market is in turmoil, and one where the market is not. Kritzman, Page, and Turkington (2012) use these two states to determine separate multi-asset allocations for ‘turbulent’ and ‘quiet’ markets and adopt a regime switching (RS) approach. To define two regimes, a metric to measure turbulence is required. The authors take a squared Mahalanobis distance (SMD) approach to determine an index through time. The Mahalanobis distance is a multi-dimensional generalisation of the notion of how many standard deviations a point is away from the mean of a distribution. The SMD index (\\(d_t\\)) is expressed mathematically as: \\[\\begin{align} d_t = (R_t - \\mu)\\Sigma^{-1}(R_t - \\mu)^\\intercal, \\end{align}\\] where: \\[d_t = \\mu_{s_t} + \\sigma_{s_t}\\epsilon_t,\\] and \\(\\epsilon_t\\) has a standard normal distribution. The state at time \\(t\\) is shown by the random variable \\(s_t\\). As asserted earlier, the two states are Quiet (\\(Q\\)) and Turbulent (\\(T\\)), hence \\(s_t \\in \\{Q, T\\}\\). To calculate the SMD, we require the unobservable inputs \\(\\Sigma\\) and \\(\\mu\\). They can be replaced by their sample counterparts, \\(\\textbf{S}\\) and \\(\\hat{\\mu}\\), to yield \\(\\hat{d}_t\\). The SMD has a state-specific mean and volatility; hence different values are observed based on the current system state. If the system emits large values of \\(\\hat{d}_t\\), the probability of being in a turbulent market is high. If the system emits small values of \\(\\hat{d}_t\\), the likelihood of being in a quiet market is high. The \\(\\zeta^{th}\\) quantile of the sample SMDs is the point where the system of reference changes. The market state is also an unobservable variable, so the above model is referred to as a hidden Markov model (HMM). The HMM used in this investigation is depicted in figure 5.1. Figure 5.1: Turbulent / quiet hidden Markov model. The transition matrix stores the probabilities of transition from a state at time \\(t\\) to another at time \\(t + 1\\) for ease of computation. It is mathematically shown as: \\[P_{t, t + 1} = \\begin{bmatrix} p_{QQ} &amp; p_{QT} \\\\ p_{TQ} &amp; p_{TT} \\end{bmatrix},\\] where the matrix applies to all times \\(t\\). Because the matrix applies to all times the system is called stationary, and the long-run probabilities of being in each state will converge. Once we have determined the most likely state at time \\(t\\) using an algorithm such as the Viterbi algorithm, we can use the series of estimated states \\(\\{\\hat{s}_t: t \\in \\{0, 1, ..., T\\}\\}\\) to partition the data history. The two datasets would be data used for the quiet sample covariance matrix and data used for the turbulent sample covariance matrix. Flint and Du Plooy (2018) blend these two sample matrices using the investor’s risk preferences and the most recent probabilities of being in each state, yielding a more sophisticated estimator of \\(\\Sigma\\). An alternative approach to dealing with heterogeneity is to focus on the assumption that errors for return forecasting models have a normal distribution. But first, we need to define a general return forecasting model. If the return of a portfolio is viewed through a set of return drivers or risk factors, then returns could be explained in part by those factors and the portfolio’s sensitivity to them. Meucci (2010) encapsulates this idea in his asset return model given below: \\[\\begin{align} R &amp;= \\alpha +\\beta^\\intercal \\mathcal{F} + \\epsilon, \\tag{5.1} \\end{align}\\] where \\(R\\) is a vector of asset returns (not portfolio returns through time as shown previously), \\(\\alpha\\) is a forecastable vector of returns unique to each security, \\(\\beta\\) is a matrix of sensitivities to risk factors, \\(\\mathcal{F}\\) is a vector of factors, and \\(\\epsilon\\) is the error vector. The error vector is assumed to have a normal distribution. Ang and Chen (2002) show that in a downward market, the correlation structure is significantly different from what is implied by a normal distribution, which is a problem when using model (5.1) in the exhibited way. Chen, Dolado, and Gonzalo (2019) address the issue of non-normal errors by utilising the quantile regression model proposed by Koenker and Bassett Jr (1978). The asset returns, idiosyncratic asset returns, asset factor sensitivities and errors could all be considered to be a function of the current quantile, denoted \\(\\tau\\). This leads to the quantile factor model (QFM): \\[\\begin{align} \\mathcal{Q}(\\tau) = \\alpha(\\tau) + \\beta(\\tau)\\mathcal{F} + \\epsilon(\\tau), \\tag{5.2} \\end{align}\\] where \\(\\tau \\in [0, 1]\\). In a different symmetric, normally distributed world, \\(\\tau\\) can be set to \\(0.5\\) and model (5.1) will be recovered. However, in the real world where symmetry and normality are often not adhered to, the quantile conditional errors can be defined more generally so that they only have to satisfy: \\[\\begin{align} \\mathbb{P}\\Big[\\epsilon(\\tau) \\leq \\underline{0} \\, \\Big | \\mathcal{F} \\Big] = \\tau \\; . \\end{align}\\] This structure emerges from the cumulative distribution function (CDF) conditional on the set of factors of each asset return \\(R_i\\). Given the conditional CDF for the returns on asset \\(i\\), \\(F_i(R_i|\\mathcal{F})\\), the quantile specific inverse CDF, \\(F^{-1}_i(\\tau| \\mathcal{F})\\), can be used to generate the quantiles \\(\\mathcal{Q}_i(\\tau)\\). Flint and Du Plooy (2018) suggest using the information about each quantile to construct a series of quantile-specific covariance matrices, which can then be blended to yield a more sophisticated estimator of \\(\\Sigma\\). Chapter insert ref to next chapter covers the implementations of these two techniques to better estimate \\(\\Sigma\\). 5.2 Penalising the optimization To add a penalty term in a way that preserves the goal of the risk optimisation, we first need to adapt the objective functions of each risk-based portfolio as given earlier in chapter @ref{rbportch}. Consider the return-targeting penalised optimisation approach of Kinn (2018), both choices of \\(f(\\cdot|\\textbf{X})\\) for the GMV and ERC portfolios can be adapted into this approach. Beginning with the GMV portfolio, Kinn views the portfolio variance as an expectation: \\[\\begin{align*} f(w, \\Sigma | \\textbf{X}) &amp;= w^\\intercal \\Sigma w &amp;\\\\ &amp; = w^\\intercal (\\mathbb{E}[{r}_t {r}_t^\\intercal] - \\mu \\mu^\\intercal) w &amp; \\small\\text{(alternate definition of $\\Sigma$)} \\\\ &amp; = \\mathbb{E}[|w^\\intercal \\mu - w^\\intercal {r}_t|^2], \\end{align*}\\] where \\({r}_t\\) represents the asset returns above the risk-free rate, and \\(\\mu\\) is a vector of the population expected excess returns as before. Rewriting the portfolio expected excess return as \\(\\bar{r} = w^\\intercal \\mu\\), the idea of return-targeting for a portfolio can be incorporated as the expectation of \\(|\\bar{r} - w^\\intercal {r}_t|^2\\), which is the squared distance to a target return level. Kinn’s approach is consistent with an MVO optimisation intuitively because the target return level is analogous an expected return constraint and minimising the return’s squared distance to this constraint is analogous to variance minimisation. The objective function can now be approximated using the sample average as a result of the law of large numbers. We still have to show how to find the GMV portfolio from an MVO procedure. As stated in table 4.1, the GMV portfolio is the MSR portfolio if the assumption of identical excess returns is met. Therefore, if the target return is set to a value that is easily obtainable \\(\\bar{r} = \\bar{r}_\\mathrm{gmv}\\), then the scheme will yield a GMV portfolio. This easily obtainable value has to be found numerically and cannot be determined a priori. The non-rigorous argument turns out to be empirically true for the portfolios analysed in this research. When the return vector is replaced by the set of sample returns \\(\\textbf{X}\\), and the expectation is approximated by the sample average, the Kinn (2018) form of objective function is recovered: \\[\\begin{align} f_{\\text{Kinn}}(w|\\textbf{X}) &amp;= \\frac{1}{T} \\sum_{t = 1}^T (\\bar{r}_\\mathrm{gmv} - \\textbf{X}^\\intercal_t w)^2, \\tag{5.3} \\end{align}\\] where \\(t\\) indexes columns of the sample returns matrix \\(\\textbf{X}\\). The GMV portfolio can be found equivalently in this way. The log-constraint4, \\(\\sum_i \\ln(w_i) \\geq c\\), can be placed on optimisation (5.3) to recover the ERC portfolio. To include the constraint in framework (3.7), a Lagrangian multiplier approach can be used to move the log-constraint to the objective function. Kinn’s adapted ERC objective function is then: \\[\\begin{align} f_{\\text{Kinn}}(w|\\textbf{X}) &amp;= \\frac{1}{T} \\sum_{t = 1}^T (\\bar{r}_{gmv} - \\textbf{X}^\\intercal_t w)^2 -\\eta_{erc}\\sum_{i = 1}^N \\ln(w_i), \\end{align}\\] where \\(\\eta_{erc}\\) is the Lagrangian multiplier scalar. Now that we have shown the standard objective functions from earlier are equivalent to the Kinn framework, we have also vindicated the general framework (3.7) as accommodative of a valid application of supervised machine learning (SML) to portfolio optimisation. Because logical choices for \\(f_{\\text{Kinn}}(\\cdot|\\textbf{X})\\) have been established, different penalty functions can be applied to the optimisation. Two common penalised regression techniques are lasso regression and ridge regression (RR). In the presence of a long-only constraint, as is applied in this research, a lasso regression does not make sense, because the penalty function is simply the sum of the absolute weights: \\(P(w) = \\sum_{i= 1}^N |w_i| =1\\). This penalty is equal to \\(1\\) for all constrained portfolios. Separately, the RR is obtained by specifying the penalty as the sum of squares for the portfolio weights: \\(P(w) = \\sum_{i= 1}^N w_i^2\\). The penalty reduces the number of admissable concentrated portfolios and intuitively is not unlike incorporating some of the EW portfolio into the ERC or GMV portfolios. Ignoring constraints, Ledoit and Wolf (2004) show that RR has the same effect as shrinking the sample covariance matrix towards the identity matrix for the GMV optimisation: \\[\\begin{align} \\textbf{S}_{RR} = \\textbf{S} + \\frac{\\lambda}{T} \\textbf{I}, \\end{align}\\] where \\(\\lambda\\) is the shrinkage intensity, and \\(T\\) is the number of sample observations. In the presence of constraints, the actual scaling factor is slightly different from Ledoit and Wolf’s calculations, but the intuition of shrinkage towards the identity matrix still applies. If \\(\\lambda\\) becomes very large, the minimum variance portfolio will tend towards the EW portfolio. Estimating lambda is thus a practical choice, and the process to do so consistently is outlined in the next chapter. 5.3 Alternate implementation methods Shen and Wang (2017) present a means to find a resampled MVO portfolio that reduces estimation risk by optimising random subsets of assets in the investment universe. The process is called subset resampling (SRS). They then aggregate resultant optimised subset portfolios to create a final `optimal’ solution. The procedure requires the inputs of a sample return matrix \\(\\textbf{X}\\) and an asset subset size \\(b\\). The subset size is related to the extent of the trade-off between bias and variance. We have to choose the degree of repeated sampling, \\(s\\), which is restricted by the available computational power. This method can be described as follows. For each of the \\(s\\) repeated samples, we randomly select the \\(j^{\\text{th}}\\) subset of \\(b\\) assets from the \\(N\\) assets in the investment universe, denoted \\(\\mathcal{I}_j\\). Using only the sample return data from the selected asset subset \\(\\textbf{X}_j\\), we then compute the associated optimal portfolios \\(\\hat{w}_j\\) using framework (3.7) and a given choice of objective function. Finally, we average the \\(s\\) optimal subset weight vectors to obtain the final optimal asset portfolio \\(\\hat{w}_\\mathrm{srs} = (\\sum_{j = 1}^s \\hat{w}_j)s^{-1}\\). The SRS process is very general, and could even be applied in conjunction with a penalised optimisation or an improved sample covariance matrix. Additionally, the user can choose the input \\(b\\). If \\(b = N\\), then the usual sample risk-based portfolio is recovered, albeit in a computationally expensive manner. If \\(b = 1\\), then the SRS procedure will yield the EW portfolio for a large enough value of \\(s\\). Therefore, \\(b\\) is the input parameter controlling the extent of the trade-off between weight diversification and estimation risk. The estimation of \\(b\\) should be done in a manner consistent with the aim of the optimisation. To ensure \\(b\\) scales with the size of the asset universe, Shen and Wang (2017) recommend writing it in the form \\(b = N^{\\alpha}\\), where \\(\\alpha \\in [0, 1]\\). The SRS method is comparable to ensemble methods in machine learning. The logical basis is that many different models can be used and aggregated into a final model, rather than assuming a single model is the most accurate to use. Despite the general nature of the SRS procedure, it is still consistent with the approach of increasing the squared bias out of the hope that the variance reduction will offset it enough to lessen overall estimation risk. Each of the three estimation risk reduction classes has at least one specific technique within them. Some modelling decisions need to be made for a prospective user to apply these techniques in an experiment. These modelling decisions are covered in the next chapter. References "],
["somemaths.html", "6 Using estimation risk reduction techniques 6.1 Quantile factor modelling and regime switching 6.2 Ridge regression", " 6 Using estimation risk reduction techniques Under the EW and MW portfolios, the portfolio weights do not require estimation whereas the GMV and ERC portfolios do. For each of these portfolios, there are six given ways of performing the estimation in this research. The first is by using the standard sample covariance matrix, the next four are to use techniques outlined in chapter 5, and the final one is to use a combination of quantile factor modelling and regime switching. A summary of all of the portfolio technique pairs is given in table 6.1 below. Table 6.1: Portfolio estimation-technique pairs Portfolio | Es Estimation technique | Abb Abbreviation Equal weight | - - | EW EW Market weight | - - | MW MW Global minimum variance | Sa Sample covariance matrix | GMV GMV-SCM Global minimum variance | Qu Quantile factor modelling | GMV GMV-QFM Global minimum variance | Re Regime switching | GMV GMV-RS Global minimum variance | Qu Quantile factor modelling with regime switching | GMV GMV-QRS Global minimum variance | Ri Ridge regression | GMV GMV-RR Global minimum variance | Su Subset re-sampling | GMV GMV-SRS Equal risk contribution | Sa Sample covariance matrix | ERC ERC-SCM Equal risk contribution | Qu Quantile factor modelling | ERC ERC-QFM Equal risk contribution | Re Regime switching | ERC ERC-RS Equal risk contribution | Qu Quantile factor modelling with regime switching | ERC ERC-QRS Equal risk contribution | Ri Ridge regression | ERC ERC-RR Equal risk contribution | Su Subset re-sampling | ERC ERC-SRS 6.1 Quantile factor modelling and regime switching QFM and RS improve the estimation of the inputs into the optimisation and therefore have the same implementation for the GMV and ERC portfolios. To use the RS model for covariance prediction, the probability of being in either a quiet or a turbulent state needs to be estimated. If \\(s_t\\) is the random variable that takes on the value of the state at time \\(t\\) then \\(s_t \\in \\{Q, T\\}\\). The true parameters are defined as: \\[\\begin{align*} \\pi_{Q, t} &amp;:= \\mathbb{P}\\{s_t = Q\\}, \\\\ \\pi_{T, t} &amp;:= \\mathbb{P}\\{s_t = T\\} = 1 - \\pi_{Q, t}. \\end{align*}\\] These can be estimated using the emission quantities from the HMM, \\(\\hat{d}_t\\), and a maximum likelihood approach - the Baum-Welch algorithm5. Kritzman, Page, and Turkington (2012) determine a turbulent-quiet data split through the parameter \\(\\zeta\\), which is the proportion of the data allocated to the quiet regime for covariance estimation. They suggest a value of \\(0.7\\) or \\(0.8\\), but due to constraints on input data length we use \\(0.7\\) in this research. The investor has nearly estimated enough parameters to blend the variance-covariance matrix using the method of Flint and Du Plooy (2018) outlined below in equation (6.1). \\[\\begin{align} \\hat{\\Sigma}_{\\text{blend}} &amp; = \\hat{\\pi}_{Q, t + 1} \\hat{\\eta}_Q \\hat{\\Sigma}_Q + \\hat{\\pi}_{T, t + 1} \\hat{\\eta}_T \\hat{\\Sigma}_T. \\tag{6.1} \\end{align}\\] The estimated probabilities, \\(\\hat{\\pi}_{Q, t + 1}\\) and \\(\\hat{\\pi}_{T, t + 1}\\), are forward-looking. Predictions can be made empirically using the current state because the transition probabilities are generally near \\(1\\) or \\(0\\) in the data. The investor still has to estimate their normalised aversion to each regime \\(\\hat{\\eta}_{s_{t+1}}\\), which they can do using the estimation procedure from Bodnar et al. (2018). For the experiment, the investor was indifferent between regimes. The intention of this blending procedure is for money managers to estimate the state probabilities themselves, thus incorporating their future beliefs. In reality, when implementing a quantitative model, it is common to use a rolling estimation window of data. Asset return data is often not long enough to accommodate sophisticated portfolio construction methods. Therefore, assigning \\(\\hat{\\pi}_{T, t + 1}\\) values near \\(1\\) results in the estimation and application of large covariance matrices with potentially only \\(30 \\%\\) of the data, without the requisite weighting of the other covariance matrix. The inaccurate covariance matrix leads to the very same input sensitivity problems that the RS model attempts to avoid. It is worth clarifying that the sensitivity is due to an incorrectly estimated \\(\\hat{\\pi}_{T, t + 1}\\), not the absence of underlying regimes. To correct this sensitivity concern, a discretised simplification is used. If the state is estimated to be quiet, nothing is done to the weights implied by the volume of data: \\[\\begin{align*} (\\hat{\\pi}_{Q, t + 1}, \\hat{\\pi}_{T, t + 1}) = (\\zeta, 1 - \\zeta). \\end{align*}\\] If the state is estimated to be turbulent, then the weights implied by the volume of data are adjusted to overweight the turbulent regime by a proportion of \\(\\gamma\\): \\[\\begin{align*} (\\hat{\\pi}_{Q, t + 1}, \\hat{\\pi}_{T, t + 1}) = (\\zeta - \\frac{\\gamma}{2}, 1 - \\zeta + \\frac{\\gamma}{2}). \\end{align*}\\] In the experiment, we set \\(\\gamma\\) to 0.1. The effect is that in the quiet regime, the covariance matrix is the same as without the RS model. While during the turbulent regime, the turbulent covariance matrix is given a weighting of \\(\\gamma\\) more than what is implied by the data split. The QFM technique is examined next. Chen, Dolado, and Gonzalo (2019) use the QFM technique from equation (5.2) for prediction. They select risk factors and estimate factor loadings using a simultaneous procedure. Factor-based modelling is not the focus of this research, although it can be used in conjunction with the general framework (3.7). In this experiment, mainly for pedagogical purposes, simple factors are used for the QFM portfolios; namely, a market factor and a squared market factor. This choice is consistent with the findings of Treynor and Mazuy (1966), with more detail given in appendix insert appendix reference here. Additionally, the QFM user has to choose how to partition the interval \\([0, 1]\\), i.e. decide which quantiles to use. The partition is an important input which could be estimated. We use the same split as Ma and Pohlman (2008)}, where: \\[[\\tau_0 = 0, \\tau_1 = 0.1]\\cup(\\tau_1 = 0.1, \\tau_2 = 0.9] \\cup (\\tau_2 = 0.9, \\tau_3 = 1].\\] In the notation, the intervals are referred to by their right endpoint. Flint and Du Plooy (2018) note that the QFM, as stated, does not provide variation between quantiles, as the idiosyncratic error term will always adjust so that the total estimated covariance matrix is always equal to the sample covariance matrix. Therefore, to estimate the quantile specific covariance matrices, they propose fixing the error term to the median error such that \\(\\hat{\\epsilon}(\\tau) = \\hat{\\epsilon}(0.5)\\). Then each set of quantile returns can be used to construct a sample covariance matrix denoted \\(\\hat{\\Sigma}^{(\\tau)}\\). Ma and Pohlman (2008) propose a strategy for portfolio forecasting called quantile regression portfolio distribution (QRPD). It involves interval-weighting the portfolio allocations \\(\\hat{w}^{(\\tau)}\\) for each quantile covariance matrix. The QRPD portfolio is: \\[\\begin{align} \\hat{w}_{\\text{QRPD}} &amp; = \\sum_{i = 1}^{l} p_i\\hat{w}^{(\\tau_i)},\\;\\; \\end{align}\\] where \\(p_i = \\tau_i - \\tau_{i - 1}\\), and \\(l\\) is the number of intervals. Like the SRS method, the QRPD method is comparable to ensemble methods because different models are being aggregated in the hope of reducing total estimation risk. The QRS technique from table 6.1 is laid out by Flint and Du Plooy (2018). They first separate the data history into regimes. Within each regime, they apply the QFM approach. Once they find \\(\\hat{\\Sigma}_{s_t}^{(\\tau_i)}\\) for every quantile and state, they blend the covariance matrices between regimes. For every possible pair of quantiles \\(\\tau_i, \\tau_j \\in \\{\\tau_1, ..., \\tau_{|\\tau|}\\}\\), a blended SCM can be constructed: \\[\\begin{align} \\hat{\\Sigma}^{(\\tau_i, \\tau_j)}_{\\text{blend}} &amp; = (\\hat{\\pi}_{Q, t + 1}\\hat{\\eta}_Q\\hat{\\Sigma}^{(\\tau_i)}_Q) + (\\hat{\\pi}_{T, t + 1}\\hat{\\eta}_T\\hat{\\Sigma}^{(\\tau_j)}_T). \\end{align}\\] Each of the SCMs can be used to find a portfolio \\(\\hat{w}^{(\\tau_i, \\tau_j)}\\). Once the \\(l \\times l\\) portfolios have been found they can also be blended with the QRPD method: \\[\\begin{align} \\hat{w}_{\\text{QRPD}} &amp; = \\sum_{i = 1}^{|\\tau|} \\sum_{j = 1}^{|\\tau|} p_i p_j \\hat{w}^{(\\tau_i, \\tau_j)}. \\end{align}\\] 6.2 Ridge regression As stated previously, the QFM and RS techniques do not require separate workarounds for the ERC and GMV portfolios. This is not the case for the RR method. First, we consider the GMV portfolio estimation and then the ERC portfolio estimation. The Lagrangian multiplier, \\(\\lambda\\), for the penalty in framework (3.7), should be chosen to minimise the estimation risk. However, the estimation risk is unobservable. Kinn (2018) suggests the k-folds cross-validation estimation procedure to select this parameter6. The broad idea of this procedure is to approximate the estimation risk and choose a value of lambda, \\(\\lambda^*\\), that minimises the approximated estimation risk. To perform k-folds cross-validation, we initially arrange the observations into \\(K\\) subsets without replacement. The subsets are taken over time and not assets as with SRS technique. The set of observations for the \\(k^{\\text{th}}\\) subset is denoted \\(\\mathcal{I}_k\\). All of the observations in the \\(K - 1\\) remaining subsets (i.e. excluding those in the \\(k^{\\text{th}}\\) subset) are stored in a set denoted \\(\\mathcal{I}_{-k}\\). This yields the optimisation: \\[\\begin{align} \\lambda^* &amp; = \\underset{\\lambda}{\\text{argmin}} \\Big \\{ \\frac{1}{K} \\sum_{k =1}^K \\hat{h}_k(\\lambda, \\mathcal{I}_k, \\mathcal{I}_{-k})\\Big \\}, \\tag{6.2} \\end{align}\\] where \\(\\hat{h}_k(\\lambda)\\) is a function that approximates estimation risk given a value of \\(\\lambda\\). One choice for \\(h\\) is the mean squared error penalty for a portfolio found using the data from \\(\\mathcal{I}_{-k}\\) and then tested on \\(\\mathcal{I}_k\\). This procedure is described below: For each subset \\(k = \\{1, 2, ..., K\\}\\), find the optimal portfolio weights using the training data \\(\\mathcal{I}_{-k}\\) and apply the penalty scaled by \\(\\lambda\\). Evaluate the performance of this portfolio on the unused data \\(\\mathcal{I}_k\\) with the mean squared error loss function and retain the score \\(\\hat{h}_k\\): \\[\\begin{align} \\hat{h}_k (\\lambda, \\mathcal{I}_k, \\mathcal{I}_{-k}) = \\frac{1}{|\\mathcal{I}_k|} \\sum_{i \\in \\mathcal{I}_k} (\\bar{r}_{gmv} - \\textbf{X}_i^\\intercal \\hat{w}_{\\mathcal{I}_{-k}}(\\lambda))^2, \\tag{6.3} \\end{align}\\] where \\(\\bar{r}_{gmv}\\) is determined as in chapter 5, and the function \\(|\\cdot|\\) counts the number of observations in a set. The range of possible \\(\\lambda\\) values can be discretised to find a solution that minimises the objective function in (6.2)7. For the ERC portfolio implementation of the RR method, there is an additional hyperparameter, \\(\\eta_{erc}\\), that needs to be estimated. This hyperparameter should be found before the penalty hyperparameter \\(\\lambda^*\\). We also use the k-folds cross-validation technique to find this parameter. However, the mean squared error loss to the desired global minimum variance portfolio from equation (6.3) is not appropriate for optimising an ERC portfolio. Ideally, \\(\\eta_{erc}^*\\) should minimise the distance between all of the total risk contributions so that they are as equal as possible. Therefore, we make use of the following hyperparameter selection function (see appendix insert reference to appendix here for motivation): \\[\\begin{align} \\hat{h}_k^{erc} (\\eta, \\mathcal{I}_k, \\mathcal{I}_{-k}) &amp; = \\sum_{i = 1}^N \\sum_{j \\geq i}^N(\\hat{w}_{i, \\mathcal{I}_{-k}}(\\textbf{S}_{\\mathcal{I}_k} \\hat{w}_{\\mathcal{I}_{-k}})_i - \\hat{w}_{j, \\mathcal{I}_{-k}}(\\textbf{S}_{\\mathcal{I}_k} \\hat{w}_{\\mathcal{I}_{-k}})_j )^2. \\end{align}\\] References "],
["experiment.html", "7 An empirical test using Sourth African equities 7.1 Data and methodology 7.2 Results", " 7 An empirical test using Sourth African equities In this chapter, we test the various approaches outlined above for constructing risk-based portfolios using South African equity data. There are two sources of variation in this experiment: variation in risk-based portfolio type and variation in estimation risk reduction technique, all combinations of which are outlined in table 6.1. Each combination of portfolio-technique approach will be referred to as a pair. 7.1 Data and methodology 7.1.1 Code to clean incoming data library(readxl) library(tidyr) library(dplyr) library(lubridate) library(magrittr) library(rlang) library(ggplot2) library(knitr) # sourcing data ------------------------------------------------------------------------------------ weekly_ret &lt;- read_xlsx( &quot;data/data_emlyn.xlsx&quot;, sheet = &quot;Weekly TRets&quot;, na = c(&quot;&quot;, &quot;NaN&quot;) ) ## New names: ## * `` -&gt; ...1 month_ret &lt;- read_xlsx( &quot;data/data_emlyn.xlsx&quot;, sheet = &quot;Monthly TRets&quot;, na = c(&quot;&quot;, &quot;NaN&quot;) ) ## New names: ## * `` -&gt; ...1 alsi_weights &lt;- read_xlsx( &quot;data/data_emlyn.xlsx&quot;, sheet = &quot;ALSI Weights&quot;, na = c(&quot;&quot;, &quot;Nan&quot;) ) ## New names: ## * `` -&gt; ...2 # cleaning data ------------------------------------------------------------------------------------ weekly_ret &lt;- weekly_ret %&gt;% rename(&quot;date&quot; = &quot;...1&quot;) %&gt;% mutate(date = as.Date(.data$date)) %&gt;% pivot_longer(setdiff(colnames(weekly_ret), c(&quot;...1&quot;))) %&gt;% drop_na() %&gt;% arrange(.data$date) %&gt;% mutate(join_col = paste0(month(date), year(date))) month_ret &lt;- month_ret %&gt;% rename(&quot;date&quot; = &quot;...1&quot;) %&gt;% mutate(date = as.Date(.data$date)) %&gt;% pivot_longer(setdiff(colnames(month_ret), c(&quot;...1&quot;))) %&gt;% drop_na() %&gt;% arrange(.data$date) %&gt;% mutate(join_col = paste0(month(date), year(date))) alsi_weights &lt;- alsi_weights %&gt;% rename(&quot;date&quot; = &quot;...2&quot;) %&gt;% select(-.data$SumCheck) %&gt;% mutate(date = as.Date(.data$date) + 15) %&gt;% # weights are at end of month, making start pivot_longer(setdiff(colnames(alsi_weights), c(&quot;SumCheck&quot;, &quot;...2&quot;))) %&gt;% drop_na() %&gt;% arrange(.data$date) %&gt;% mutate(join_col = paste0(month(date), year(date))) # joining data ------------------------------------------------------------------------------------- full_ret &lt;- weekly_ret %&gt;% left_join( month_ret, by = c(&quot;join_col&quot;, &quot;name&quot;), suffix = c(&quot;_week&quot;, &quot;_month&quot;) ) %&gt;% select( date = .data$date_week, .data$name, week_ret = .data$value_week, month_ret = .data$value_month, .data$join_col ) %&gt;% left_join( alsi_weights, by = c(&quot;join_col&quot;, &quot;name&quot;), suffix = c(&quot;_full&quot;, &quot;_weights&quot;) ) %&gt;% select( date = .data$date_full, .data$name, .data$week_ret, .data$month_ret, month_weight_start = .data$value, month_index = .data$join_col ) # keeping complete cases only ---------------------------------------------------------------------- complete_per &lt;- sum(complete.cases(full_ret))/nrow(full_ret) # data is 98.2% complete incomp_weight_per &lt;- sum(full_ret[!complete.cases(full_ret),]$month_weight_start, na.rm = TRUE) / sum(full_ret$month_weight_start, na.rm = TRUE) # but only 0.2% of the weight is in the incomplete data, so it is largely # isolated to the long tail of small stocks that are outside of the # top 40 and 100 and are therefore not needed in this research full_ret &lt;- full_ret[complete.cases(full_ret),] # plot coverage full_ret %&gt;% filter(.data$month_weight_start &gt;= 0) %&gt;% group_by(date) %&gt;% count() %&gt;% ggplot(aes(.data$date, .data$n)) + geom_line() + ggtitle(&quot;coverage through time&quot;) # getting indices ---------------------------------------------------------------------------------- top40_month_weights &lt;- full_ret %&gt;% group_by(.data$month_index) %&gt;% filter(.data$date == min(.data$date)) %&gt;% mutate(size_order = rank(-.data$month_weight_start)) %&gt;% filter(.data$size_order &lt;= 40) %&gt;% mutate(top40_weight = .data$month_weight_start / sum(.data$month_weight_start)) %&gt;% ungroup() %&gt;% select(.data$month_index, .data$name, .data$top40_weight) top100_month_weights &lt;- full_ret %&gt;% group_by(.data$month_index) %&gt;% filter(.data$date == min(.data$date)) %&gt;% mutate(size_order = rank(-.data$month_weight_start)) %&gt;% filter(.data$size_order &lt;= 100) %&gt;% mutate(top100_weight = .data$month_weight_start / sum(.data$month_weight_start)) %&gt;% ungroup() %&gt;% select(.data$month_index, .data$name, .data$top100_weight) full_ret &lt;- full_ret %&gt;% full_join( top40_month_weights, by = c(&quot;month_index&quot;, &quot;name&quot;) ) %&gt;% full_join( top100_month_weights, by = c(&quot;month_index&quot;, &quot;name&quot;) ) %&gt;% select( .data$date, .data$name, .data$week_ret, .data$month_ret, .data$top40_weight, .data$top100_weight, .data$month_index ) %&gt;% replace_na(list(top40_weight = 0, top100_weight = 0)) saveRDS(full_ret, file = &quot;data/full_ret.rds&quot;) head(full_ret, 10) ## # A tibble: 10 x 7 ## date name week_ret month_ret top40_weight top100_weight month_index ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1996-02-02 ABI -0.0314 0.0133 0 0.00104 21996 ## 2 1996-02-02 AXL 0.0436 0.0436 0 0 21996 ## 3 1996-02-02 ACL -0.0886 0.0218 0 0 21996 ## 4 1996-02-02 ACT 0 0.0833 0 0 21996 ## 5 1996-02-02 ADR 0 0.250 0 0.0000499 21996 ## 6 1996-02-02 AEL 0.0208 -0.265 0 0 21996 ## 7 1996-02-02 AEN 0.0455 -0.130 0 0 21996 ## 8 1996-02-02 AFE -0.0545 -0.106 0.00626 0.00598 21996 ## 9 1996-02-02 AFI 0 0.105 0 0.00109 21996 ## 10 1996-02-02 AFL 0 0 0 0 21996 7.1.2 Comments on data The available data for this experiment are weekly total-returns for all equity stocks included in the Johannesburg Stock Exchange (JSE) All Share Index (ALSI) over the period ranging from the \\(5^{th}\\) January 1996 to the \\(1^{st}\\) November 2019. Weightings of the shares in the ALSI are also available, although this is a monthly series. This historical vector of the weights of the MW portfolio through time is denoted \\(\\hat{w}^{\\text{mw}}_t\\). The investigation uses a rolling estimation window of \\(T = 400\\)8 weekly observations to construct portfolios. Weekly data inputs are used with monthly rebalancing because of the high data volume requirement from chapter 6. The constructed portfolio \\(\\hat{w}_{t_i}\\) will be held out-of-sample over the interval \\([t_i, t_{i + 1}]\\), where \\(t_i\\) denotes a month’s end. The entire holding period is from the \\(30^{th}\\) of September 2003 to the \\(27^{th}\\) of September 2019 a total of 16 years. Portfolios are constructed for two different asset universes; namely, the \\(40\\) largest stocks and the \\(100\\) largest stocks. The separation is to test the effect on portfolio performance of an investor broadening their universe. This methodology is outlined for each rebalancing date: Of the stocks in the ALSI with a data history of at least length \\(T\\), choose the \\(N\\) stocks with the largest market weights. Use the \\(N \\times T\\) matrix of sample returns to find asset weights for the relevant portfolio-technique pair. Hold the found portfolio \\(\\hat{w}\\) for one month, record the returns \\(\\hat{r}\\), and calculate the portfolio turnover. 7.1.3 Data preparation for rebalance dats library(rlang) library(magrittr) library(dplyr) library(data.table) library(tidyr) full_ret &lt;- readRDS(&quot;data/full_ret.rds&quot;) sample_est_wind &lt;- 400 rebal_est_ranges &lt;- full_ret %&gt;% select(.data$date, .data$month_index) %&gt;% distinct() %&gt;% mutate(start_date = shift(.data$date, sample_est_wind)) %&gt;% mutate(end_date = shift(.data$date, 1)) %&gt;% na.omit() %&gt;% group_by(.data$month_index) %&gt;% filter(.data$date == min(.data$date)) %&gt;% ungroup() %&gt;% select(.data$month_index, .data$start_date, .data$end_date) model_rebal_dat &lt;- tibble(month_index = rebal_est_ranges$month_index, data = list(NULL)) for (i in seq_along(rebal_est_ranges$month_index)) { dat_now &lt;- full_ret %&gt;% filter(.data$date &lt;= rebal_est_ranges[i, ]$end_date &amp; .data$date &gt;= rebal_est_ranges[i, ]$start_date) %&gt;% mutate(month_index = rebal_est_ranges[i, ]$month_index) %&gt;% nest(data = c(&quot;date&quot;, &quot;name&quot;, &quot;week_ret&quot;, &quot;month_ret&quot;, &quot;top40_weight&quot;, &quot;top100_weight&quot;)) model_rebal_dat[i, ] &lt;- dat_now[1, ] } saveRDS(model_rebal_dat, file = &quot;data/model_rebal_dat.rds&quot;) head(model_rebal_dat, 6) ## # A tibble: 6 x 2 ## month_index data ## &lt;chr&gt; &lt;list&gt; ## 1 102003 &lt;tibble [75,749 × 6]&gt; ## 2 112003 &lt;tibble [76,103 × 6]&gt; ## 3 122003 &lt;tibble [76,373 × 6]&gt; ## 4 12004 &lt;tibble [76,634 × 6]&gt; ## 5 22004 &lt;tibble [76,948 × 6]&gt; ## 6 32004 &lt;tibble [77,186 × 6]&gt; 7.1.4 Limitations This experiment is specific to this data and method. Therefore, it is important to outline some limitations of the results. Although framework (3.7) is broad, this experiment only analyses portfolios constructed with South African equity returns, and cannot be used to state facts about the portfolio construction process generally. The budget, long-only and maximum weight constraints were all applied for this experiment. Specifically the maximum weight constraint used is \\(\\alpha = 0.1\\). The choice intends to provide a consistent basis for comparison across portfolio-technique pairs and ensures similarity to practical applications. Shares that have missing data were excluded at each rebalancing date with a filter, therefore the MW portfolio, where \\(N = 40\\) may not be representative of the JSE Top 40 index. Transaction costs are ignored due to the liquid nature of the largest stocks, although this is a potential area for future improvement on the backtesting methodology. 7.2 Results Before the results are reported, specific metrics that illustrate the effect of each technique and portfolio need to be introduced. The first is turnover (TO). It measures the magnitude of trading required on each rebalancing date. If there are \\(m\\) rebalancing dates, then the TO is calculated as: \\[\\begin{align} \\text{TO} &amp; = \\frac{6}{(m - 1)} \\sum_{i = 1}^{m - 1} \\sum_{j =1}^{N} |\\hat{w}_{t_i, j}^\\triangle - \\hat{w}_{t_i, j}|, \\end{align}\\] where \\(\\hat{w}_{t_i, j}^\\triangle\\) is the buy-and-hold weight of the \\(j^{th}\\) asset just before the rebalance at time \\(t_i\\). The turnover is annual and one-way only; hence, the scaling factor of \\(\\frac{6}{m-1}\\). For a higher TO, the investor has increased risk when rebalancing that they will not enter into the new portfolio at the current market price. The maximum possible value of the TO is \\(1200\\%\\), and the investor achieves it if they switch from one single stock portfolio to another at every rebalancing date. The next reporting metric is maximum drawdown (MDD), the definition of which requires the notion of cumulative wealth. The investor’s cumulative wealth at time \\(t_j\\) is their return from time \\(t_0\\) until time \\(t_j\\) on a portfolio of one initial unit investment, and it is defined as: \\[\\begin{align*} W_{t_j} &amp;= \\prod_{i = 1}^{j} (1 + \\hat{r}_{t_i}), \\end{align*}\\] where \\(\\hat{r}_{t_i}\\) is the return realised over the interval \\([t_{i - 1}, t_i)\\). The MDD is the biggest loss in cumulative wealth for the entire investment period, and is formulated as: \\[\\begin{align} \\text{MDD} &amp; = \\underset{t_j \\in \\{t_1, ..., t_m\\}}{\\text{argmax}} \\Big \\{ \\underset{t_i \\in \\{t_1, \\text{...}, t_j\\}}{\\text{argmax} \\{W_{t_i} \\}} - W_{t_j} \\Big \\} \\; . \\end{align}\\] MDD is an essential measure for money managers because excessive drawdowns lead to redemptions in their funds. (Magdon-Ismail and Atiya 2004) The last two measures are measures of concentration. As stated earlier, the risk-based investor is making a trade-off between weight concentration and risk concentration. A standard for weight concentration at a rebalancing date, the inverse Herfindahl index, has already been defined. But the IHI is general and could apply to multiple types of weights. For allocation weights the IHI at time \\(t_i\\) is given as: \\[\\begin{align} N^{\\text{eff}}_{t_i} &amp;= \\frac{1}{\\sum_{j = 1}^N w_{t_i, j}^2} \\; , \\end{align}\\] where $N^{}{t_i} $ can be interpreted as the number of equal-weighted stocks the investor’s portfolio is equivalent to. The notation reflects the idea that the IHI represents the number of effective stocks in the portfolio. The upper bound of $N^{}{t_i} $ is \\(N\\) and the lower bound in the presence of a maximum allocation constraint is: \\(({\\lfloor \\frac{1}{\\alpha} \\rfloor \\cdot \\alpha^2 + (1- \\lfloor \\frac{1}{\\alpha}\\rfloor \\cdot \\alpha)^2})^{-1}\\) , which is derived in appendix B.2. For the case when \\(\\alpha = 0.1\\) the lower bound is \\(10\\). Risk-weights can be taken instead of capital weights to measure the effectiveness of an asset on portfolio volatility. Since the TRCs sum to the portfolio volatility, the risk contributions can be scaled into risk-weights so that they sum to \\(1\\). The risk-weight for the \\(j^{th}\\) asset at time \\(t_i\\) is given as: \\[\\begin{align} \\text{RW}_{t_i, j} &amp;= \\frac{\\text{TRC}_{t_i, j}}{\\sqrt{w_{t_i}^\\intercal \\Sigma w_{t_i}}}\\; , \\end{align}\\] where the inverse Herfindahl index approach can be applied again. The risk-weight IHI is written as: \\[\\begin{align} N^\\mathrm{rw}_{t_i}(RW_{t_i}) &amp;= \\frac{1}{\\sum_{j = 1}^N \\text{RW}_{t_i, j}^2} \\;. \\end{align}\\] If \\(\\Sigma\\) is positive semi-definite, then the lower bound is \\(1\\) even in the presence of the maximum allocation constraint, because one asset could have a positive weight and considerable volatility. The upper bound is once again \\(N\\). The concentration measures are reported as averages across all of the \\(m\\) rebalancing dates. Let us first discuss the results for the GMV portfolios. The main objective of constructing GMV portfolios is to reduce the out-of-sample volatility of returns. The main aim of each GMV-technique pair is the same, but performance is measured against the GMV-SCM pair. Therefore, it makes sense to define a measure of each GMV-technique pair performance in the same way as Richard and Roncalli (2015), as a volatility reduction over the GMV-SCM pair: \\[\\begin{align} \\mathcal{VR}(w|w_{\\text{gmv-scm}}) &amp; = \\frac{\\sigma(w_{\\text{gmv-scm}}) - \\sigma(w)}{\\sigma(w_{\\text{gmv-scm}})}, \\end{align}\\] where the \\(\\sigma(\\cdot)\\) function measures a portfolio’s volatility. 7.2.1 going to R’ify References "],
["conclusion.html", "8 Conclusion 8.1 Avenues for further research", " 8 Conclusion This dissertation introduces a flexible framework for risk-based investing that is a general tool and allows the user to incorporate different estimation techniques into their risk-based portfolio construction. The framework and risk-based investing are empirically sound when tested with South African data, as all risk-based portfolios outperformed the market weight portfolio using the Sharpe ratio measure. The superiority of risk-based portfolios is consistent with similar results for US equities found by DeMiguel, Garlappi, and Uppal (2007) and Kritzman, Page, and Turkington (2010). Within risk-based portfolios, we found that GMV portfolios performed the best using the same Sharpe ratio measure. Furthermore, broadening the asset universe to include more assets materially improved portfolio performance. No techniques outperformed the standard sample covariance matrix technique for finding risk-based portfolios across all risk-based portfolios, which is consistent with the findings of Kritzman, Page, and Turkington (2010). However, they propose longer estimation windows (20-years or 10-years) than were used in this study. Due to the similar performance of shrinkage techniques and standard sample estimation techniques, the period of this study (7.69-years) is close to an empirical break-even performance point between sample estimation and lack-of-data adjusted estimation strategies. Quantile factor modelling with regime switching worked well for the smaller and more concentrated GMV portfolios, where estimating a small number of substantial allocations correctly is essential. Subset resampling performed well for the less concentrated ERC portfolios, where determining the greatest number of allocations is necessary. Ridge regression shrinkage techniques have historically performed well in other studies. Still, the long-only constraint and a restrictive maximum allocation constraint of 10% have the effect of regularising the optimisation in a manner already shown by JM03. Therefore, the impact of ridge regression on performance is diminished. 8.1 Avenues for further research Avenues for further research include using the flexible investing framework with other risk-based portfolios and penalising the objective function with alternate functions. The structure may also be modified to include different constraints. Further experiments may also be performed with varying universes of assets. Additionally, the general portfolio estimation procedure could be performed in risk factor space instead of the asset space. References "],
["abbreviations.html", "A Abbreviations", " A Abbreviations Table A.1: Portfolio type abbreviations. Full name | Abb Abbrevation | Mean-variance optimal | MVO MVO | Global minimum variance | GMV GMV | Equal weight | EW EW | Equal risk contribution | ERC ERC | Market weight | MW MW | Maximum Sharpe ratio | MSR MSR | Table A.2: Metric abbreviations. Full name Abbrevation | Marginal risk contribution MRC | Total risk contribution TRC | Inverse Herfindahl index IHI | Risk-weights RS | Marginal risk-weights MRW | Total risk-weights TRW | Sharpe ratio SR | Squared Mahalanobis distance SMD | Mean square error MSE | Turnover rate TO | Maximum drawdown MDD | Table A.3: Estimation techniques. Full name | Abb Abbrevation | Sample covariance matrix | SCM SCM | Regime switching | RS RS | Quantile factor modelling | QFM QFM | Quantile factor modelling with regime switching | QRS QRS | Ridge regression | RR RR | Subset re-sampling | SRS SRS | "],
["portfolio-mathematics.html", "B Portfolio mathematics B.1 Marginal risk contribution B.2 Minimum weight constrained IHI B.3 Proof of the volatility order inequality B.4 Maximum Sharpe ratio risk-based portfolios", " B Portfolio mathematics B.1 Marginal risk contribution The portfolio beta interpretation of MRC: \\[\\begin{align*} \\frac{\\partial}{\\partial w_i} \\Big [ \\sigma(w) \\Big ] &amp; = \\frac{\\partial}{\\partial w_i} \\Big [ (w^\\intercal \\Sigma w)^{\\frac{1}{2}} \\Big ], &amp; \\\\ &amp; = \\frac{1}{2} \\cdot (w^\\intercal \\Sigma w)^{-\\frac{1}{2}} \\cdot \\frac{\\partial}{\\partial w_i} \\Big [ w^\\intercal \\Sigma w \\Big ], &amp; \\text{(chain rule)} \\\\ &amp; = \\frac{\\frac{1}{2}\\cdot 2 \\cdot (\\Sigma w)_i}{\\sqrt{w^\\intercal \\Sigma w}}, &amp; \\text{(as given in the main text)} \\\\ &amp; = \\frac{\\mathbb{COV}[R_i, R_p]}{\\sqrt{w^\\intercal \\Sigma w}}, &amp; \\text{($R_i$- returns on asset i)} \\\\ &amp; = \\mathbb{SD}[(R_p)] \\cdot \\frac{\\mathbb{COV}[R_i, R_p]}{\\mathbb{VAR}[(R_p)]}, &amp; \\\\ &amp; = \\mathbb{SD}[(R_p)] \\cdot \\beta_{i, p}\\;. &amp; \\end{align*}\\] Minimising the weighted sum of the of the MRC’s is equivalently the lowest beta portfolio with the single risk factor of the portfolio itself, which is the GMV portfolio. \\[\\begin{align*} \\sum_{i = 1}^N w_i \\cdot \\frac{\\partial}{\\partial w_i} \\Big [ \\sigma(w) \\Big ] &amp; = \\frac{\\sum_{i = 1}^N w_i \\cdot (\\Sigma w)_i}{\\sqrt{w^\\intercal \\Sigma w}}, &amp; \\\\ \\underset{w}{\\text{argmin}} \\Big \\{\\sum_{i = 1}^N w_i \\cdot \\frac{\\partial}{\\partial w_i} \\Big [ \\sigma(w) \\Big ] \\Big \\} &amp; = \\underset{w}{\\text{argmin}} \\Big \\{ \\sqrt{w^\\intercal \\Sigma w} \\Big\\},&amp; \\text{(Taking the minimum on both sides)}\\\\ &amp; = \\underset{w}{\\text{argmin}} \\Big \\{w^\\intercal \\Sigma w \\} . &amp; \\text{($\\sqrt{\\cdot}$ monotonic)}\\\\ \\end{align*}\\] B.2 Minimum weight constrained IHI The maximum possible weight allocation for a single asset is \\(\\alpha\\). This allocation should be given to \\(\\lfloor \\frac{1}{\\alpha} \\rfloor\\) assets where \\(\\lfloor \\cdot \\rfloor\\) is the truncation function. An additional weight \\(j\\) should be set to satisfy the budget constraint, hence it should be \\(w_j = 1 - \\lfloor \\frac{1}{\\alpha} \\rfloor \\cdot \\alpha\\). All of the other weights should be set to 0. Then the lower bound is: \\[\\begin{align*} H_{\\text{lb}}^{-1} &amp; = \\frac{1}{\\lfloor \\frac{1}{\\alpha} \\rfloor \\cdot \\alpha^2 + (1- \\lfloor \\frac{1}{\\alpha}\\rfloor \\cdot \\alpha)^2} \\;\\; . \\end{align*}\\] B.3 Proof of the volatility order inequality Consider a GMV framework where the IHI is bounded from below by a constant \\(c\\): \\[\\begin{align*} w^\\diamond = \\underset{w}{\\text{argmin}} \\Big \\{ w^\\intercal \\Sigma w \\Big \\}, \\end{align*}\\] subject to the constraints: \\[\\begin{align*} \\mathcal{C}(w) &amp;= \\begin{cases} H^{-1}(w) \\geq c \\\\ w^\\intercal \\underline{1} = 1 \\;\\; \\\\ 0 \\leq w_i \\leq \\alpha, \\; \\forall i \\;\\;\\; . \\end{cases} \\end{align*}\\] If \\(c \\leq H^{-1}_{lb}\\) then the constraint has no effect and the GMV portfolio is recovered. If \\(c = N\\) then the only feasible solution is the EW portfolio. If \\(c &gt; N\\) there are no feasible solutions to the problem. The portfolio volatility is an increasing function of \\(c\\) therefore we can deduce that: \\[\\sigma(w_{gmv}) \\leq \\sigma(w^\\diamond(c)) \\leq \\sigma (w_{ew}).\\] It remains to show that the ERC portfolio is a \\(w^\\diamond\\) portfolio. Consider replacing the IHI constraint above with a log-constraint, \\(\\sum_i \\ln(w_i) \\geq c\\). If \\(c = - \\infty\\) then the GMV portfolio is recovered. If \\(c = -n \\ln(n)\\) then the EW portfolio is recovered, and if \\(c &gt; -n\\ln(n)\\) then there are no feasible solutions. The portfolio volatility is also an increasing monotonic function of c therefore the inequality is replicated for a scaled choice of \\(c\\) and by extension the ERC portfolio. B.4 Maximum Sharpe ratio risk-based portfolios As mentioned in the text Scherer (2007) has shown that the marginal sharpe ratios are equalised for the MSR portfolio as below: \\[\\begin{align} \\frac{\\mu_i}{\\text{MRC}_i} &amp; = \\frac{\\mu_j}{\\text{MRC}_j} \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\}, \\tag{B.1} \\end{align}\\] where \\(\\mu_k\\) represents the marginal excess return of asset k. Separately Jurczenko, Michel, and Teiletche (2013) showed that the EW, GMV and ERC portfolios could be found by the equalisation strategy: \\[\\begin{align} w_i^\\gamma \\sigma^{-\\delta}_i \\text{MRC}_i &amp; = w_j^\\gamma \\sigma^{-\\delta}_j \\text{MRC}_j \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\}, \\tag{B.2} \\end{align}\\] where the choice of \\(\\gamma\\) and \\(\\delta\\) defines the portfolio. Combining the portfolio condition from equality (B.2) with the optimality equality (B.1) yields the optimality condition for a given portfolio: \\[\\begin{align} w_i^\\gamma \\sigma_i^{(1- \\delta)} \\text{SR}_i &amp; = w_j^\\gamma \\sigma_j^{(1- \\delta)} \\text{SR}_j \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\}, \\end{align}\\] where \\(\\text{SR}_k = \\frac{\\mu_k}{\\sigma_k}\\). Hence, a risk-based portfolio is optimal when constituents have equal weighted risk-adjusted SRs. The EW portfolio can be analysed by setting \\((\\gamma, \\delta) = (\\infty, 0)\\). For equlities (B.1) and (B.2) to be jointly true when \\(w_i = \\frac{1}{N}\\) then: \\[\\text{MRC}_i = \\text{MRC}_j \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\},\\] and there needs to be identical excess returns between assets. This means there should also be identical volatilities and identical correlations between all assets (i.e. \\(\\Sigma = \\rho\\sigma^2 \\underline{1} \\,\\underline{1}^\\intercal - \\rho I\\)). The GMV portfolio can be analysed by setting \\((\\gamma, \\delta) = (0,0)\\), which yields the optimality condition: \\[\\mu_i = \\mu_j \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\}.\\] Hence, identical excess returns are required only. The ERC portfolio can be analysed by setting \\((\\gamma, \\delta) = (1, 0)\\), which means the optimality condition becomes: \\[w_i \\sigma_i \\text{SR}_i = w_j \\sigma_j \\text{SR}_j \\;\\;\\; \\forall i, j \\in \\{1, ..., N \\}.\\] Assuming constant correlation, \\(\\rho_{i, j} = \\rho\\), the ERC portfolio allocations are given by the weighted inverse volatilities in the portfolio \\(w_i = \\frac{\\sigma^{-1}_i}{\\sum_{k = 1}^N \\sigma^{-1}_k}\\) Maillard, Roncalli, and Teı̈letche (2010). The equality of SRs is then the requirement for the above shown condition to hold. Therefore, identical correlations and SRs ensure that the ERC portfolio coincides with the MSR portfolio. References "],
["reducing-estimation-error.html", "C Reducing estimation error C.1 Quantile factor modelling example C.2 Estimating the ERC portfolio Lagrangian multiplier", " C Reducing estimation error C.1 Quantile factor modelling example Consider the QFM (from equation (5.2)) with two factors, the market risk factor \\(R_m\\) and the squares of that factor as an additional factor \\(R_m^2\\). The factor loadings can be estimated by a procedure outlined by Koenker and Bassett Jr (1978). Once the quantile errors have been deduced, the quantile factor sensitivities have been estimated, and the error quantiles set to the median quantile as in the main text the following quantile prediction model can be used: \\[\\begin{align} \\hat{\\mathcal{Q}}(\\tau) = \\hat{\\alpha} (\\tau) + \\hat{\\beta}_1^\\intercal(\\tau)R_m + \\hat{\\beta}_2^\\intercal(\\tau)R_m^2 + \\epsilon_t(0.5), \\end{align}\\] which yields \\(|\\tau|\\) sample covariance matrices \\(\\hat{\\Sigma}^{(\\tau)}\\). C.2 Estimating the ERC portfolio Lagrangian multiplier We want to choose a value of \\(\\eta\\) that minimises all of the distances between the total risk contributions so that they are minimised, but this should be done across all \\(K\\) folds using cross-validation. The general optimisation is the same as before: \\[\\begin{align} \\eta^* = \\underset{\\eta}{\\text{argmin}} \\Big \\{ \\frac{1}{K} \\sum_{k = 1}^K \\hat{h}_k^{erc}(\\eta, \\mathcal{I}_k, \\mathcal{I}_{-k})\\Big \\}. \\end{align}\\] The procedure of estimating \\(h\\) still needs to be specified. The function \\(f(\\cdot|\\textbf{X})\\) used to find the ERC portfolio can be used as a distance to minimise. Hence: \\[\\begin{align} h_k^{erc}(\\eta, \\mathcal{I}_k, \\mathcal{I}_{-k}) = \\sum_{i = 1}^N \\sum_{j \\geq i}^N(w_{i, \\mathcal{I}_{-k}}(\\Sigma_{\\mathcal{I}_k} w_{\\mathcal{I}_{-k}})_i - w_{j, \\mathcal{I}_{-k}}(\\Sigma_{\\mathcal{I}_k} w_{\\mathcal{I}_{-k}})_j )^2, \\end{align}\\] where \\(w\\) and \\(\\Sigma\\) can be replaced by their sample estimations to find \\(\\hat{h}\\) given in the main text. References "],
["references.html", "References", " References Ang, Andrew. 2014. Asset Management: A Systematic Approach to Factor Investing. Oxford University Press. Ang, Andrew, and Joseph Chen. 2002. “Asymmetric Correlations of Equity Portfolios.” Journal of Financial Economics 63 (3): 443–94. Best, Michael J, and Robert R Grauer. 1991. “Sensitivity Analysis for Mean-Variance Portfolio Problems.” Management Science 37 (8): 980–89. Bodnar, Taras, Yarema Okhrin, Valdemar Vitlinskyy, and Taras Zabolotskyy. 2018. “Determination and Estimation of Risk Aversion Coefficients.” Computational Management Science 15 (2): 297–317. Chen, Liang, J. Juan Dolado, and Jesus Gonzalo. 2019. “Quantile Factor Models.” Clarke, Roger, Harindra De Silva, and Steven Thorley. 2011. “Minimum-Variance Portfolio Composition.” The Journal of Portfolio Management 37 (2): 31–45. Clarke, Roger G, Harindra De Silva, and Steven Thorley. 2006. “Minimum-Variance Portfolios in the Us Equity Market.” The Journal of Portfolio Management 33 (1): 10–24. DeMiguel, Victor, Lorenzo Garlappi, and Raman Uppal. 2007. “Optimal Versus Naive Diversification: How Inefficient Is the 1/N Portfolio Strategy?” The Review of Financial Studies 22 (5): 1915–53. Flint, Emlyn James, and Simon Du Plooy. 2018. “Extending Risk Budgeting for Market Regimes and Quantile Factor Models.” Available at SSRN 3141739. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Vol. 1. 10. Springer series in statistics New York. Hadamard, J. 1923. Lectures on Chacy’s Problem in Linear Partial Differential Equations. Yale University Press. Haugen, Robert A, and Nardin L Baker. 1991. “The Efficient Market Inefficiency of Capitalization–Weighted Stock Portfolios.” The Journal of Portfolio Management 17 (3): 35–40. Jagannathan, Ravi, and Tongshu Ma. 2003. “Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps.” The Journal of Finance 58 (4): 1651–83. Jobson, J David, and Robert M Korkie. 1981. “Putting Markowitz Theory to Work.” The Journal of Portfolio Management 7 (4): 70–74. Jurczenko, Emmanuel, Thierry Michel, and Jerome Teiletche. 2013. “Generalized Risk-Based Investing.” Available at SSRN 2205979. Kinn, Daniel. 2018. “Reducing Estimation Risk in Mean-Variance Portfolios with Machine Learning.” arXiv Preprint arXiv:1804.01764. Koenker, Roger, and Gilbert Bassett Jr. 1978. “Regression Quantiles.” Econometrica: Journal of the Econometric Society, 33–50. Kritzman, Mark, Sebastien Page, and David Turkington. 2012. “Regime Shifts: Implications for Dynamic Strategies (Corrected).” Financial Analysts Journal 68 (3): 22–39. Kritzman, Mark, Sébastien Page, and David Turkington. 2010. “In Defense of Optimization: The Fallacy of 1/N.” Financial Analysts Journal 66 (2): 31–39. Ledoit, Olivier, and Michael Wolf. 2004. “Honey, I Shrunk the Sample Covariance Matrix.” The Journal of Portfolio Management 30 (4): 110–19. Ma, Lingjie, and Larry Pohlman. 2008. “Return Forecasts and Optimal Portfolio Construction: A Quantile Regression Approach.” The European Journal of Finance 14 (5): 409–25. Magdon-Ismail, Malik, and Amir F Atiya. 2004. “Maximum Drawdown.” Risk Magazine 17 (10): 99–102. Maillard, Sébastien, Thierry Roncalli, and Jérôme Teı̈letche. 2010. “The Properties of Equally Weighted Risk Contribution Portfolios.” The Journal of Portfolio Management 36 (4): 60–70. Markowitz, Harry. 1952. “Portfolio Selection.” The Journal of Finance 7 (1): 77–91. Meucci, Attilio. 2010. “Factors on Demand: Building a Platform for Portfolio Managers, Risk Managers and Traders.” Risk 23 (7): 84–89. Michaud, Richard O. 1989. “The Markowitz Optimization Enigma: Is ’Optimized’optimal?” Financial Analysts Journal 45 (1): 31–42. Perold, André F. 2007. “Fundamentally Flawed Indexing.” Financial Analysts Journal 63 (6): 31–37. Plessis, Hannes du, and Paul van Rensburg. 2017. “Diversification and the Realised Volatility of Equity Portfolios.” Investment Analysts Journal 46 (3): 213–34. Rabiner, Lawrence R. 1989. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” In Proc. IEEE. Richard, Jean-Charles, and Thierry Roncalli. 2015. “Smart Beta: Managing Diversification of Minimum Variance Portfolios.” In Risk-Based and Factor Investing, 31–63. Elsevier. Scherer, Bernd. 2007. “Can Robust Portfolio Optimisation Help to Build Better Portfolios?” Journal of Asset Management 7 (6): 374–87. Sharpe, William F. 1964. “Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk.” The Journal of Finance 19 (3): 425–42. Shen, Weiwei, and Jun Wang. 2017. “Portfolio Selection via Subset Resampling.” In Thirty-First Aaai Conference on Artificial Intelligence. Treynor, Jack, and Kay Mazuy. 1966. “Can Mutual Funds Outguess the Market.” Harvard Business Review 44 (4): 131–36. "]
]
